{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ded05db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from cca_zoo.deepmodels import (\n",
    "    DCCA,\n",
    "    DCCA_NOI,\n",
    "    DCCA_SDL,\n",
    "    #BarlowTwins,\n",
    "    get_dataloaders,\n",
    "    \n",
    ")\n",
    "from cca_zoo.deepmodels.utils import architectures, objectives\n",
    "from cca_zoo.plotting import pairplot_label\n",
    "from cca_zoo.data import CCA_Dataset\n",
    "from cca_zoo.models import CCA\n",
    "from cca_zoo.model_selection import GridSearchCV\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import fisher_exact\n",
    "from scipy.stats.contingency import crosstab\n",
    "from scipy.stats import hypergeom\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch import nn\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "from ray.tune.integration.pytorch_lightning import tune\n",
    "os.chdir('../raw_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b5dcf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Files:\n",
    "    def __init__(self, filename):\n",
    "        self.file = filename\n",
    "        \n",
    "    def write_to_file(self, data):\n",
    "        with open(self.file, 'wb') as f:\n",
    "            pickle.dump(data, f) \n",
    "        return None\n",
    "    \n",
    "    def load_pickle(self):\n",
    "        data = pd.read_pickle(self.file)\n",
    "        return data\n",
    "    \n",
    "    def load_csv(self, sep, usecols=None):\n",
    "        data = pd.read_csv(self.file, sep=sep, usecols=usecols)\n",
    "        return data\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f475c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load df with all info\n",
    "path = './df_classes_max3_embeddings.pickle'\n",
    "df_all = Files(path).load_pickle()[:100]#just a subset for scripting\n",
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0e52552",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCCA:\n",
    "    def __init__(self, df_all,batch_size = 768,num_workers = 6,\\\n",
    "                latent_dims=100, epochs=300, lr=0.001): #default dims determined after iteratin 10:50 dims \n",
    "        \n",
    "        self.df_all = df_all\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.v1='ms2ds'\n",
    "        self.v2 = 'mol2vec'\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        self.sdl_lr = 0.025118864315095822#0.01#lr (picked after running lr_finder)\n",
    "        self.dcca_lr = 5.623413251903491e-08 #lr\n",
    "        self.latent_dims=latent_dims\n",
    "        self.optim = 'sgd'\n",
    "        self.activation = nn.Tanh()\n",
    "        self.objective = objectives.CCA\n",
    "        self.encoder_1_layers = (500,500)\n",
    "        self.encoder_2_layers = (500,500)\n",
    "        seed_everything(15)\n",
    "        \n",
    "        \n",
    "    def split_data(self,test_size=0.2,\\\n",
    "                   random_state=None,stratify=None): # thinking of removing this one\n",
    "        \n",
    "        if random_state != None and stratify == None:\n",
    "            train_df, test_df = \\\n",
    "            train_test_split(self.df_all, test_size=test_size, random_state=random_state)\n",
    "        \n",
    "        elif random_state == None and stratify != None:\n",
    "            train_df, test_df = \\\n",
    "            train_test_split(self.df_all, test_size=test_size,stratify=self.df_all[stratify])\n",
    "        else:\n",
    "            train_df, test_df = \\\n",
    "            train_test_split(self.df_all, test_size=test_size, random_state=42)\n",
    "        \n",
    "        return train_df, test_df \n",
    "    \n",
    "    def gen_views(self,v1='ms2ds',v2='mol2vec'):\n",
    "        \n",
    "        #split test, train\n",
    "        train_df, test_df= self.split_data(test_size=0.2,random_state=42)\n",
    "        \n",
    "        #Split train dataset into train and validation set\n",
    "        train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "        \n",
    "        \n",
    "        #extract the 2 view, v1 == spectra embeddings, v2==structure embeddings\n",
    "        v1_train, v1_test = np.array([x for x in train_df[v1]]), np.array([x for x in test_df[v1]])\n",
    "        v2_train, v2_test = np.array([x for x in train_df[v2]]), np.array([x for x in test_df[v2]])\n",
    "\n",
    "        # validation\n",
    "        v1_val, v2_val = np.array([x for x in val_df[v1]]), np.array([x for x in val_df[v2]])\n",
    "        \n",
    "        #update self dfs\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.val_df = val_df\n",
    "        \n",
    "        # not memory efficient !!!\n",
    "        self.v1_train, self.v1_test = v1_train, v1_test\n",
    "        self.v2_train, self.v2_test = v2_train, v2_test\n",
    "        self.v1_val, self.v2_val = v1_val, v2_val\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def deepcca_encoders(self):\n",
    "        # define encoders\n",
    "        self.encoder_1 = architectures.Encoder(\n",
    "                                      latent_dims = self.latent_dims, \n",
    "                                      feature_size = self.v1_size,\n",
    "                                      layer_sizes = self.encoder_1_layers,\n",
    "                                      activation = self.activation\n",
    "                                    )\n",
    "                                     \n",
    "        self.encoder_2 = architectures.Encoder(\n",
    "                                      latent_dims=self.latent_dims, \n",
    "                                      feature_size=self.v2_size, \n",
    "                                      layer_sizes=self.encoder_2_layers,\n",
    "                                      activation = self.activation\n",
    "                                     )\n",
    "       \n",
    "        return None#[encoder_1, encoder_2]\n",
    "        \n",
    "    \n",
    "    def deepcca_dataloaders(self):\n",
    "        \n",
    "        #v1_train,v1_test, v2_train,v2_test, v1_val, v2_val = \\\n",
    "        self.gen_views(v1=self.v1, v2=self.v2)\n",
    "        \n",
    "        #creat CCA dataset \n",
    "        train_dataset = CCA_Dataset([self.v1_train, self.v2_train])\n",
    "        test_dataset = CCA_Dataset([self.v1_test, self.v2_test])\n",
    "        val_dataset = CCA_Dataset([self.v1_val, self.v2_val])\n",
    "        \n",
    "        #update features size\n",
    "        self.v1_size = self.v1_train.shape[1]\n",
    "        self.v2_size = self.v2_train.shape[1]\n",
    "        self.N = len(train_dataset)\n",
    "        \n",
    "        #set N (for sdl; equal len train dataset)\n",
    "        self.N = len(train_dataset)\n",
    "        \n",
    "        #loaders\n",
    "        self.train_loader , self.val_loader = get_dataloaders(train_dataset, \\\n",
    "                                                    val_dataset,batch_size=self.batch_size,\\\n",
    "                                                    num_workers=self.num_workers,drop_last=False)\n",
    "        self.test_loader = get_dataloaders(test_dataset,\\\n",
    "                                      batch_size=self.batch_size, \\\n",
    "                                      num_workers=self.num_workers,drop_last=False)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def train_cca(self): #only for comparison with other deep models\n",
    "        \n",
    "       \n",
    "        \n",
    "        #define and train cca model\n",
    "        print('\\n','-'*20,'\\n Training CCA\\n','-'*20)\n",
    "        cca = CCA(latent_dims=self.latent_dims).fit((self.v1_train, self.v2_train))\n",
    "        self.cca = cca\n",
    "        return None#cca\n",
    "    \n",
    "    def train_sdl(self, checkpoint=None, logger=None,lam=0.0001,enable_progress_bar=True ):\n",
    "        \n",
    "        \n",
    "        # 2. SDL\n",
    "        sdl = DCCA_SDL(self.latent_dims,\n",
    "                       optimizer=self.optim,\n",
    "                       N=self.N, \n",
    "                       encoders = [self.encoder_1,self.encoder_2],\n",
    "                       lam=0.0001, \n",
    "                       lr=self.sdl_lr,\n",
    "                       dropout=0.05,\n",
    "                       objective=self.objective) \n",
    "\n",
    "        \n",
    "        \n",
    "        #define the trainer\n",
    "        \n",
    "        self.trainer = pl.Trainer(#default_root_dir=default_root_dir,\n",
    "                             logger = logger,\n",
    "                             max_epochs=self.epochs, #enable early stoppage instead of specifiying num epochs                           \n",
    "                             log_every_n_steps=1,\n",
    "                             val_check_interval = 1, #`Trainer(val_check_interval=1)` was configured so validation will run after every batch.\n",
    "                             \n",
    "                             callbacks=[\n",
    "                                checkpoint,\n",
    "                                 #pl.callbacks.early_stopping.EarlyStopping(monitor=\"val/l2\") # early stopage to reduce overfitting\n",
    "                             ],\n",
    "                            enable_progress_bar=enable_progress_bar,\n",
    "                            auto_lr_find = True\n",
    "                                )#,\n",
    "        \n",
    "        #callbacks=[pl.callbacks.early_stopping.EarlyStopping(monitor=\"train/sdl\")])# early stopage to reduce overfitting\n",
    "        \n",
    "        print('\\n','-'*20,'\\n Training SDL\\n','-'*20)\n",
    "        self.trainer.fit(sdl, self.train_loader,self.val_loader)\n",
    "        self.sdl = sdl\n",
    "        return None#sdl\n",
    "    \n",
    "    def train_dcca(self):\n",
    "        \n",
    "       \n",
    "        \n",
    "        # 2. DCCA\n",
    "        dcca = DCCA(self.latent_dims,\n",
    "                    optimizer=self.optim,\n",
    "                    encoders = [self.encoder_1,self.encoder_2],\n",
    "                    lr=self.dcca_lr,\n",
    "                    objective=self.objective) \n",
    "\n",
    "        #train\n",
    "        #tb_logger = pl_loggers.TensorBoardLogger(save_dir=\"pl_logs/dcca\")\n",
    "        trainer = pl.Trainer(default_root_dir=\"./dcca\",max_epochs=self.epochs,log_every_n_steps=1)#,\n",
    "        \n",
    "        #callbacks=[pl.callbacks.early_stopping.EarlyStopping(monitor=\"train/sdl\")])# early stopage to reduce overfitting\n",
    "\n",
    "        print('\\n','-'*20,'\\n Training DCCA\\n','-'*20)\n",
    "        trainer.fit(dcca, self.train_loader,self.val_loader)\n",
    "        \n",
    "        self.dcca = dcca\n",
    "        \n",
    "        return None #dcca\n",
    "    \n",
    "    \n",
    "    \n",
    "    def score(self,model,dataset): \n",
    "        \"\"\"\n",
    "        model: either 'cca', 'dcca', 'sdl'\n",
    "        dataset: 'train', 'test', or 'val'\n",
    "        \n",
    "        returns: correlation \n",
    "        \"\"\"\n",
    "       # for cca models \n",
    "        #m = eval(model)\n",
    "        \n",
    "        #specify data to transform\n",
    "        if dataset == 'train':\n",
    "            v1,v2, loader = self.v1_train, self.v2_train, self.train_loader\n",
    "        elif dataset == 'test':\n",
    "            v1,v2, loader = self.v1_test, self.v2_test, self.test_loader\n",
    "        elif dataset == 'val':\n",
    "            v1,v2, loader = self.v1_val, self.v2_val, self.val_loader\n",
    "        \n",
    "        if model == 'cca':\n",
    "            corr = self.cca.score([v1,v2])\n",
    "        \n",
    "        if model == 'sdl':\n",
    "            corr = self.sdl.score(loader)\n",
    "        \n",
    "        elif model == 'dcca':\n",
    "            corr = self.dcca.score(loader)\n",
    "       \n",
    "        return corr\n",
    "    def update_z_scores(self,dataset, z1,z2,cols):\n",
    "        #update train df with transformed z scores\n",
    "            if dataset == 'train':\n",
    "                \n",
    "                self.train_df[cols[0]] = [x for x in z1]\n",
    "                self.train_df[cols[1]] = [x for x in z2]\n",
    "                \n",
    "            #update test df\n",
    "            if dataset == 'test':\n",
    "                self.test_df[cols[0]] = [x for x in z1]\n",
    "                self.test_df[cols[1]] = [x for x in z2]\n",
    "            \n",
    "            #update val df\n",
    "            if dataset == 'val':\n",
    "                self.val_df[cols[0]] = [x for x in z1]\n",
    "                self.val_df[cols[1]] = [x for x in z2]\n",
    "            return None\n",
    "                \n",
    "        \n",
    "    \n",
    "    def transform(self,model,dataset):\n",
    "        \"\"\"\n",
    "        model: either 'cca', 'dcca', 'sdl': of course the model must have been fitted :)\n",
    "        loader: is similar data loader used to train either sdl/dcca\n",
    "        dataset: either 'train', 'test', 'val'\n",
    "        \n",
    "        returns transformed data; view1,view2\n",
    "        \"\"\"\n",
    "        \n",
    "        #specify data to transform\n",
    "        if dataset == 'train':\n",
    "            v1,v2, loader = self.v1_train, self.v2_train, self.train_loader\n",
    "        elif dataset == 'test':\n",
    "            v1,v2, loader = self.v1_test, self.v2_test, self.test_loader\n",
    "        elif dataset == 'val':\n",
    "            v1,v2, loader = self.v1_val, self.v2_val, self.val_loader\n",
    "        \n",
    "        \n",
    "        #specify the model for transformation\n",
    "        if model == 'cca':\n",
    "            z1,z2 = self.cca.transform([v1,v2]) #transform\n",
    "            self.update_z_scores(dataset,z1,z2,cols=['cca_z1','cca_z2']) # update the df with z scores            \n",
    "    \n",
    "            \n",
    "        if model == 'sdl':\n",
    "            z1,z2 = self.sdl.transform(loader)\n",
    "            self.update_z_scores(dataset,z1,z2,cols=['sdl_z1','sdl_z2'])\n",
    "                \n",
    "        \n",
    "        if model == 'dcca':\n",
    "            z1,z2 = self.dcca.transform(loader)\n",
    "            self.update_z_scores(dataset,z1,z2,cols=['dcca_z1','dcca_z2'])\n",
    "        \n",
    "        \n",
    "        return None##z1,z2; can be found in self.<df[model_z]>       \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a54ecaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 15\n"
     ]
    }
   ],
   "source": [
    "# Initiate deepcca objec\n",
    "Models = DeepCCA(df_all)\n",
    "\n",
    "# generate data loaders and cca v1,v2\n",
    "Models.deepcca_dataloaders()\n",
    "\n",
    "# set up the encoders\n",
    "Models.deepcca_encoders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27f5d508",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#define the metric to monitor 'like the scorer for gridsearch'\n",
    "\n",
    "metrics = {\"loss\": \"val/l2\"}\n",
    "\n",
    "#creat a callback that will communicate with ray-tune\n",
    "ray_tune_callback = TuneReportCallback(metrics, on=\"validation_end\")\n",
    "\n",
    "# Defining a search space!\n",
    "config = {\n",
    "     \"optimizer\": tune.choice(['sgd', 'adam', 'adamw']),\n",
    " \n",
    "     \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "     \"batch_size\": tune.choice([128, 128*2, 128*3]),\n",
    "    \"latent_dims\": tune.choice([10,20,30]), \n",
    "    \"dropout\": tune.choice([0.05,0.1,0.15,0.2,0.25]),\n",
    "    \"activation\":tune.choice([\n",
    "            nn.LeakyReLU(), # already the default in cca_zoo\n",
    "            nn.ReLU(),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Tanh()\n",
    "            ])\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c63ee115",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up logger\n",
    "version =  f'testing_ray_tune'\n",
    "   \n",
    "experiment_dir = './sdl_logs'\n",
    "    \n",
    "checkpoint = ModelCheckpoint(save_last=True,\n",
    "                                       monitor=\"val/l2\",\n",
    "                                       mode = 'min')\n",
    "    \n",
    "logger = TensorBoardLogger(save_dir=experiment_dir, \n",
    "                                   name='ray_tune',\n",
    "                                   version = version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2297f8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tune(config, epochs=10, gpus=0):\n",
    "    \n",
    "    \n",
    "    #set up logger\n",
    "    version =  f'testing_ray_tune'\n",
    "   \n",
    "    experiment_dir = './sdl_logs'\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(save_last=True,\n",
    "                                       monitor=\"val/l2\",\n",
    "                                       mode = 'min')\n",
    "    \n",
    "    logger = TensorBoardLogger(save_dir=experiment_dir, \n",
    "                                   name='ray_tune',\n",
    "                                   version = version)\n",
    "    \n",
    "    Models.latent_dims = config['latent_dims']\n",
    "    \n",
    "    Models.activation = config['activation'] \n",
    "    \n",
    "    # set up the encoders with the new params\n",
    "    Models.deepcca_encoders()\n",
    "\n",
    "    \n",
    "    sdl = DCCA_SDL(Models.latent_dims,\n",
    "                optimizer=config['optimizer'],\n",
    "                N=Models.N, \n",
    "                encoders = [Models.encoder_1,Models.encoder_2],\n",
    "                lam=0.0001, \n",
    "                lr=Models.sdl_lr,\n",
    "                dropout=0.05,\n",
    "                objective=Models.objective)\n",
    "    \n",
    "    trainer = pl.Trainer(#default_root_dir=default_root_dir,\n",
    "    logger = logger,\n",
    "    max_epochs=10, #enable early stoppage instead of specifiying num epochs                           \n",
    "    log_every_n_steps=1,\n",
    "    val_check_interval = 1, #`Trainer(val_check_interval=1)` was configured so validation will run after every batch.\n",
    "    callbacks=[\n",
    "        checkpoint,\n",
    "        pl.callbacks.early_stopping.EarlyStopping(monitor=\"val/l2\"), # early stopage to reduce overfitting\n",
    "        ray_tune_callback\n",
    "              ],\n",
    "    enable_progress_bar=True,\n",
    "    auto_lr_find = True\n",
    "    )\n",
    "    \n",
    "    trainer.fit(sdl,Models.train_loader,Models.val_loader)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ae6702a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir=~/ray_results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4bda1c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-01-10 12:20:44</td></tr>\n",
       "<tr><td>Running for: </td><td>00:02:14.17        </td></tr>\n",
       "<tr><td>Memory:      </td><td>4.3/15.4 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/4.47 GiB heap, 0.0/2.23 GiB objects\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc            </th><th>activation          </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout</th><th style=\"text-align: right;\">  latent_dims</th><th style=\"text-align: right;\">        lr</th><th>optimizer  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_tune_83467_00000</td><td>TERMINATED</td><td>127.0.0.1:5956 </td><td>LeakyReLU(negat_ffa0</td><td style=\"text-align: right;\">         384</td><td style=\"text-align: right;\">     0.1 </td><td style=\"text-align: right;\">           30</td><td style=\"text-align: right;\">0.0989989 </td><td>sgd        </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         87.093 </td><td style=\"text-align: right;\"> 1.07836</td></tr>\n",
       "<tr><td>train_tune_83467_00001</td><td>TERMINATED</td><td>127.0.0.1:17152</td><td>Tanh()              </td><td style=\"text-align: right;\">         384</td><td style=\"text-align: right;\">     0.05</td><td style=\"text-align: right;\">           10</td><td style=\"text-align: right;\">0.00338178</td><td>adam       </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         80.5888</td><td style=\"text-align: right;\">16.4923 </td></tr>\n",
       "<tr><td>train_tune_83467_00002</td><td>TERMINATED</td><td>127.0.0.1:8588 </td><td>LeakyReLU(negat_fd90</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">     0.15</td><td style=\"text-align: right;\">           30</td><td style=\"text-align: right;\">0.00129123</td><td>sgd        </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         80.3961</td><td style=\"text-align: right;\"> 1.3501 </td></tr>\n",
       "<tr><td>train_tune_83467_00003</td><td>TERMINATED</td><td>127.0.0.1:10572</td><td>ReLU()              </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">     0.25</td><td style=\"text-align: right;\">           20</td><td style=\"text-align: right;\">0.0290575 </td><td>adam       </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         80.5436</td><td style=\"text-align: right;\">21.0767 </td></tr>\n",
       "<tr><td>train_tune_83467_00004</td><td>TERMINATED</td><td>127.0.0.1:16212</td><td>LeakyReLU(negat_4490</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">     0.25</td><td style=\"text-align: right;\">           20</td><td style=\"text-align: right;\">0.0121429 </td><td>adam       </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         78.6366</td><td style=\"text-align: right;\">28.8088 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m `Trainer(val_check_interval=1)` was configured so validation will run after every batch.\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:354: LightningDeprecationWarning: The `on_configure_sharded_model` callback hook was deprecated in v1.6 and will be removed in v1.8. Use `setup()` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:359: LightningDeprecationWarning: The `on_before_accelerator_backend_setup` callback hook was deprecated in v1.6 and will be removed in v1.8. Use `setup()` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:364: LightningDeprecationWarning: `TuneReportCallback.on_load_checkpoint` will change its signature and behavior in v1.8. If you wish to load the state of the callback, use `load_state_dict` instead. In v1.8 `on_load_checkpoint(..., checkpoint)` will receive the entire loaded checkpoint dictionary instead of callback state.\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_start` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_end` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m   | Name     | Type       | Params\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m ----------------------------------------\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m 0 | encoders | ModuleList | 782 K \n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m 1 | mse      | MSELoss    | 0     \n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m 2 | bns      | ModuleList | 0     \n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m ----------------------------------------\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m 782 K     Trainable params\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m 782 K     Total params\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m 3.128     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]\n",
      "Sanity Checking DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 31.83it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m `Trainer(val_check_interval=1)` was configured so validation will run after every batch.\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:354: LightningDeprecationWarning: The `on_configure_sharded_model` callback hook was deprecated in v1.6 and will be removed in v1.8. Use `setup()` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:359: LightningDeprecationWarning: The `on_before_accelerator_backend_setup` callback hook was deprecated in v1.6 and will be removed in v1.8. Use `setup()` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:364: LightningDeprecationWarning: `TuneReportCallback.on_load_checkpoint` will change its signature and behavior in v1.8. If you wish to load the state of the callback, use `load_state_dict` instead. In v1.8 `on_load_checkpoint(..., checkpoint)` will receive the entire loaded checkpoint dictionary instead of callback state.\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_start` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_end` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m   | Name     | Type       | Params\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m ----------------------------------------\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m 0 | encoders | ModuleList | 762 K \n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m 1 | mse      | MSELoss    | 0     \n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m 2 | bns      | ModuleList | 0     \n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m ----------------------------------------\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m 762 K     Trainable params\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m 762 K     Total params\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m 3.048     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s] \n",
      "Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s]                             \n",
      "Sanity Checking DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 45.14it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m `Trainer(val_check_interval=1)` was configured so validation will run after every batch.\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:354: LightningDeprecationWarning: The `on_configure_sharded_model` callback hook was deprecated in v1.6 and will be removed in v1.8. Use `setup()` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:359: LightningDeprecationWarning: The `on_before_accelerator_backend_setup` callback hook was deprecated in v1.6 and will be removed in v1.8. Use `setup()` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:364: LightningDeprecationWarning: `TuneReportCallback.on_load_checkpoint` will change its signature and behavior in v1.8. If you wish to load the state of the callback, use `load_state_dict` instead. In v1.8 `on_load_checkpoint(..., checkpoint)` will receive the entire loaded checkpoint dictionary instead of callback state.\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_start` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_end` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m   | Name     | Type       | Params\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m ----------------------------------------\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m 0 | encoders | ModuleList | 782 K \n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m 1 | mse      | MSELoss    | 0     \n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m 2 | bns      | ModuleList | 0     \n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m ----------------------------------------\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m 782 K     Trainable params\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m 782 K     Total params\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m 3.128     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]\n",
      "Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s]                             \n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 62.48it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m `Trainer(val_check_interval=1)` was configured so validation will run after every batch.\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:354: LightningDeprecationWarning: The `on_configure_sharded_model` callback hook was deprecated in v1.6 and will be removed in v1.8. Use `setup()` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:359: LightningDeprecationWarning: The `on_before_accelerator_backend_setup` callback hook was deprecated in v1.6 and will be removed in v1.8. Use `setup()` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:364: LightningDeprecationWarning: `TuneReportCallback.on_load_checkpoint` will change its signature and behavior in v1.8. If you wish to load the state of the callback, use `load_state_dict` instead. In v1.8 `on_load_checkpoint(..., checkpoint)` will receive the entire loaded checkpoint dictionary instead of callback state.\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_start` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_end` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m   | Name     | Type       | Params\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m ----------------------------------------\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m 0 | encoders | ModuleList | 772 K \n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m 1 | mse      | MSELoss    | 0     \n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m 2 | bns      | ModuleList | 0     \n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m ----------------------------------------\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m 772 K     Trainable params\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m 772 K     Total params\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m 3.088     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s] \n",
      "Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s]                             \n",
      "Sanity Checking DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 62.34it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m `Trainer(val_check_interval=1)` was configured so validation will run after every batch.\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:354: LightningDeprecationWarning: The `on_configure_sharded_model` callback hook was deprecated in v1.6 and will be removed in v1.8. Use `setup()` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:359: LightningDeprecationWarning: The `on_before_accelerator_backend_setup` callback hook was deprecated in v1.6 and will be removed in v1.8. Use `setup()` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:364: LightningDeprecationWarning: `TuneReportCallback.on_load_checkpoint` will change its signature and behavior in v1.8. If you wish to load the state of the callback, use `load_state_dict` instead. In v1.8 `on_load_checkpoint(..., checkpoint)` will receive the entire loaded checkpoint dictionary instead of callback state.\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_start` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_end` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m   | Name     | Type       | Params\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m ----------------------------------------\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m 0 | encoders | ModuleList | 772 K \n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m 1 | mse      | MSELoss    | 0     \n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m 2 | bns      | ModuleList | 0     \n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m ----------------------------------------\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m 772 K     Trainable params\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m 772 K     Total params\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m 3.088     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s] \n",
      "Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s]                             \n",
      "Sanity Checking DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 95.05it/s]\n",
      "Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s]                             \n",
      "Epoch 0:  50%|█████     | 1/2 [00:58<00:58, 58.64s/it, loss=2.03, v_num=tune, train/objective=2.030, train/l2=2.020, train/sdl=56.60]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m 2023-01-10 12:19:55.870864: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m 2023-01-10 12:19:55.871393: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m \r",
      "Epoch 0:  50%|█████     | 1/2 [00:58<00:58, 58.43s/it]\r",
      "Epoch 0:  50%|█████     | 1/2 [00:58<00:58, 58.43s/it]\r",
      "Epoch 0:  50%|█████     | 1/2 [00:58<00:58, 58.43s/it, loss=1.92, v_num=tune, train/objective=1.920, train/l2=1.910, train/sdl=20.00]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m 2023-01-10 12:20:04.614511: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m 2023-01-10 12:20:04.614843: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m \r",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>date               </th><th>done  </th><th>episodes_total  </th><th>experiment_id                   </th><th>experiment_tag                                                                                                  </th><th>hostname  </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">    loss</th><th>node_ip  </th><th style=\"text-align: right;\">  pid</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th>timesteps_total  </th><th style=\"text-align: right;\">  training_iteration</th><th style=\"text-align: right;\">   trial_id</th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_tune_83467_00000</td><td>2023-01-10_12-20-09</td><td>True  </td><td>                </td><td>c03ef941841b433d8062a15a4656e0d4</td><td>0_activation=LeakyReLU_negative_slope_0_01,batch_size=384,dropout=0.1000,latent_dims=30,lr=0.0990,optimizer=sgd </td><td>LLL       </td><td style=\"text-align: right;\">                        10</td><td style=\"text-align: right;\"> 1.07836</td><td>127.0.0.1</td><td style=\"text-align: right;\"> 5956</td><td style=\"text-align: right;\">             87.093 </td><td style=\"text-align: right;\">         0.109351 </td><td style=\"text-align: right;\">       87.093 </td><td style=\"text-align: right;\"> 1673349609</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">83467_00000</td><td style=\"text-align: right;\">    0.0155997</td></tr>\n",
       "<tr><td>train_tune_83467_00001</td><td>2023-01-10_12-20-12</td><td>True  </td><td>                </td><td>4a5fcbb0abe14d58b1333407a30f3764</td><td>1_activation=Tanh,batch_size=384,dropout=0.0500,latent_dims=10,lr=0.0034,optimizer=adam                         </td><td>LLL       </td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">16.4923 </td><td>127.0.0.1</td><td style=\"text-align: right;\">17152</td><td style=\"text-align: right;\">             80.5888</td><td style=\"text-align: right;\">         0.145987 </td><td style=\"text-align: right;\">       80.5888</td><td style=\"text-align: right;\"> 1673349612</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">83467_00001</td><td style=\"text-align: right;\">    0.0156171</td></tr>\n",
       "<tr><td>train_tune_83467_00002</td><td>2023-01-10_12-20-21</td><td>True  </td><td>                </td><td>2478e959c45f44548b8ffd03b8d58fd7</td><td>2_activation=LeakyReLU_negative_slope_0_01,batch_size=128,dropout=0.1500,latent_dims=30,lr=0.0013,optimizer=sgd </td><td>LLL       </td><td style=\"text-align: right;\">                        10</td><td style=\"text-align: right;\"> 1.3501 </td><td>127.0.0.1</td><td style=\"text-align: right;\"> 8588</td><td style=\"text-align: right;\">             80.3961</td><td style=\"text-align: right;\">         0.0942624</td><td style=\"text-align: right;\">       80.3961</td><td style=\"text-align: right;\"> 1673349621</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">83467_00002</td><td style=\"text-align: right;\">    0.0138371</td></tr>\n",
       "<tr><td>train_tune_83467_00003</td><td>2023-01-10_12-20-32</td><td>True  </td><td>                </td><td>e8613ab2c52849c78b51d2620eee3e38</td><td>3_activation=ReLU,batch_size=128,dropout=0.2500,latent_dims=20,lr=0.0291,optimizer=adam                         </td><td>LLL       </td><td style=\"text-align: right;\">                        10</td><td style=\"text-align: right;\">21.0767 </td><td>127.0.0.1</td><td style=\"text-align: right;\">10572</td><td style=\"text-align: right;\">             80.5436</td><td style=\"text-align: right;\">         0.117228 </td><td style=\"text-align: right;\">       80.5436</td><td style=\"text-align: right;\"> 1673349632</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">83467_00003</td><td style=\"text-align: right;\">    0.0156507</td></tr>\n",
       "<tr><td>train_tune_83467_00004</td><td>2023-01-10_12-20-42</td><td>True  </td><td>                </td><td>953c180ac82b424d8f7b041c31bec8bb</td><td>4_activation=LeakyReLU_negative_slope_0_01,batch_size=256,dropout=0.2500,latent_dims=20,lr=0.0121,optimizer=adam</td><td>LLL       </td><td style=\"text-align: right;\">                        10</td><td style=\"text-align: right;\">28.8088 </td><td>127.0.0.1</td><td style=\"text-align: right;\">16212</td><td style=\"text-align: right;\">             78.6366</td><td style=\"text-align: right;\">         0.156699 </td><td style=\"text-align: right;\">       78.6366</td><td style=\"text-align: right;\"> 1673349642</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">83467_00004</td><td style=\"text-align: right;\">    0.0312788</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 2/2 [01:12<00:00, 36.06s/it, loss=2.03, v_num=tune, train/objective=2.030, train/l2=2.020, train/sdl=56.60]\n",
      "Epoch 0: 100%|██████████| 2/2 [01:12<00:00, 36.17s/it, loss=2.03, v_num=tune, train/objective=2.030, train/l2=2.020, train/sdl=56.60]\n",
      "Epoch 0: 100%|██████████| 2/2 [01:12<00:00, 36.18s/it, loss=2.03, v_num=tune, train/objective=2.030, train/l2=2.020, train/sdl=56.60]\n",
      "Epoch 1:  50%|█████     | 1/2 [00:00<00:00, 13.52it/s, loss=1.66, v_num=tune, train/objective=1.290, train/l2=1.290, train/sdl=53.20]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[Am \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 2/2 [00:00<00:00, 22.77it/s, loss=1.66, v_num=tune, train/objective=1.290, train/l2=1.290, train/sdl=53.20]\n",
      "Epoch 1: 100%|██████████| 2/2 [00:00<00:00, 17.37it/s, loss=1.66, v_num=tune, train/objective=1.290, train/l2=1.290, train/sdl=53.20]\n",
      "                                                              \u001b[A\n",
      "Epoch 1: 100%|██████████| 2/2 [00:00<00:00, 16.24it/s, loss=1.66, v_num=tune, train/objective=1.290, train/l2=1.290, train/sdl=53.20]\n",
      "Epoch 2:  50%|█████     | 1/2 [00:00<00:00, 31.58it/s, loss=1.44, v_num=tune, train/objective=0.988, train/l2=0.982, train/sdl=66.90]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[Am \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 2/2 [00:00<00:00, 23.55it/s, loss=1.44, v_num=tune, train/objective=0.988, train/l2=0.982, train/sdl=66.90]\n",
      "Epoch 2: 100%|██████████| 2/2 [00:00<00:00, 23.55it/s, loss=1.44, v_num=tune, train/objective=0.988, train/l2=0.982, train/sdl=66.90]\n",
      "Epoch 3:  50%|█████     | 1/2 [00:00<00:00, 31.59it/s, loss=1.27, v_num=tune, train/objective=0.788, train/l2=0.781, train/sdl=71.30]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[Am \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 2/2 [00:00<00:00, 23.93it/s, loss=1.27, v_num=tune, train/objective=0.788, train/l2=0.781, train/sdl=71.30]\n",
      "Epoch 4:   0%|          | 0/2 [00:00<?, ?it/s, loss=1.27, v_num=tune, train/objective=0.788, train/l2=0.781, train/sdl=71.30]        \n",
      "Epoch 4:  50%|█████     | 1/2 [00:00<00:00, 31.59it/s, loss=1.16, v_num=tune, train/objective=0.729, train/l2=0.721, train/sdl=77.90]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[Am \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 2/2 [00:00<00:00, 31.78it/s, loss=1.16, v_num=tune, train/objective=0.729, train/l2=0.721, train/sdl=77.90]\n",
      "Epoch 5:  50%|█████     | 1/2 [00:00<00:00, 31.29it/s, loss=1.07, v_num=tune, train/objective=0.619, train/l2=0.611, train/sdl=84.70]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[Am \n",
      "\u001b[2m\u001b[36m(train_tune pid=5956)\u001b[0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 2/2 [00:00<00:00, 50.09it/s, loss=1.07, v_num=tune, train/objective=0.619, train/l2=0.611, train/sdl=84.70]\n",
      "Epoch 5: 100%|██████████| 2/2 [00:00<00:00, 31.21it/s, loss=1.07, v_num=tune, train/objective=0.619, train/l2=0.611, train/sdl=84.70]\n",
      "Epoch 6:  50%|█████     | 1/2 [00:00<00:00, 21.66it/s, loss=0.995, v_num=tune, train/objective=0.523, train/l2=0.514, train/sdl=87.40]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[Am \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 2/2 [00:00<00:00, 32.17it/s, loss=0.995, v_num=tune, train/objective=0.523, train/l2=0.514, train/sdl=87.40]\n",
      "Epoch 6: 100%|██████████| 2/2 [00:00<00:00, 18.10it/s, loss=0.995, v_num=tune, train/objective=0.523, train/l2=0.514, train/sdl=87.40]\n",
      "Epoch 7:  50%|█████     | 1/2 [00:00<00:00, 30.56it/s, loss=0.929, v_num=tune, train/objective=0.469, train/l2=0.460, train/sdl=90.40]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[Am \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 2/2 [00:00<00:00, 41.06it/s, loss=0.929, v_num=tune, train/objective=0.469, train/l2=0.460, train/sdl=90.40]\n",
      "Epoch 7: 100%|██████████| 2/2 [00:00<00:00, 26.73it/s, loss=0.929, v_num=tune, train/objective=0.469, train/l2=0.460, train/sdl=90.40]\n",
      "Epoch 8:  50%|█████     | 1/2 [00:00<00:00, 28.17it/s, loss=0.872, v_num=tune, train/objective=0.417, train/l2=0.408, train/sdl=92.10]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[Am \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 2/2 [00:00<00:00, 37.19it/s, loss=0.872, v_num=tune, train/objective=0.417, train/l2=0.408, train/sdl=92.10]\n",
      "Epoch 8: 100%|██████████| 2/2 [00:00<00:00, 25.27it/s, loss=0.872, v_num=tune, train/objective=0.417, train/l2=0.408, train/sdl=92.10]\n",
      "Epoch 8: 100%|██████████| 2/2 [00:00<00:00, 25.27it/s, loss=0.872, v_num=tune, train/objective=0.417, train/l2=0.408, train/sdl=92.10]\n",
      "Epoch 9:  50%|█████     | 1/2 [00:00<00:00, 32.00it/s, loss=0.822, v_num=tune, train/objective=0.366, train/l2=0.357, train/sdl=95.20]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[Am \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 2/2 [00:00<00:00, 26.20it/s, loss=0.822, v_num=tune, train/objective=0.366, train/l2=0.357, train/sdl=95.20]\n",
      "Epoch 9: 100%|██████████| 2/2 [00:00<00:00, 26.20it/s, loss=0.822, v_num=tune, train/objective=0.366, train/l2=0.357, train/sdl=95.20]\n",
      "Epoch 9: 100%|██████████| 2/2 [00:00<00:00, 16.39it/s, loss=0.822, v_num=tune, train/objective=0.366, train/l2=0.357, train/sdl=95.20]\n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 2/2 [01:05<00:00, 32.71s/it, loss=1.92, v_num=tune, train/objective=1.920, train/l2=1.910, train/sdl=20.00]\n",
      "Epoch 0: 100%|██████████| 2/2 [01:05<00:00, 32.71s/it, loss=1.92, v_num=tune, train/objective=1.920, train/l2=1.910, train/sdl=20.00]\n",
      "Epoch 1:   0%|          | 0/2 [00:00<?, ?it/s, loss=1.92, v_num=tune, train/objective=1.920, train/l2=1.910, train/sdl=20.00]        \n",
      "Epoch 1:  50%|█████     | 1/2 [00:00<00:00,  7.45it/s, loss=1.92, v_num=tune, train/objective=1.920, train/l2=1.920, train/sdl=39.90]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 2/2 [00:00<00:00, 11.96it/s, loss=1.92, v_num=tune, train/objective=1.920, train/l2=1.920, train/sdl=39.90]\n",
      "Epoch 1: 100%|██████████| 2/2 [00:00<00:00, 11.42it/s, loss=1.92, v_num=tune, train/objective=1.920, train/l2=1.920, train/sdl=39.90]\n",
      "Epoch 2:  50%|█████     | 1/2 [00:00<00:00, 20.10it/s, loss=1.85, v_num=tune, train/objective=1.710, train/l2=1.700, train/sdl=63.80]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 2/2 [00:00<00:00, 33.32it/s, loss=1.85, v_num=tune, train/objective=1.710, train/l2=1.700, train/sdl=63.80]\n",
      "Epoch 2: 100%|██████████| 2/2 [00:00<00:00, 23.43it/s, loss=1.85, v_num=tune, train/objective=1.710, train/l2=1.700, train/sdl=63.80]\n",
      "                                                              \u001b[A\n",
      "Epoch 2: 100%|██████████| 2/2 [00:00<00:00, 23.43it/s, loss=1.85, v_num=tune, train/objective=1.710, train/l2=1.700, train/sdl=63.80]\n",
      "Epoch 3:  50%|█████     | 1/2 [00:00<00:00, 20.35it/s, loss=1.67, v_num=tune, train/objective=1.140, train/l2=1.130, train/sdl=81.30]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 2/2 [00:00<00:00, 24.27it/s, loss=1.67, v_num=tune, train/objective=1.140, train/l2=1.130, train/sdl=81.30]\n",
      "Epoch 3: 100%|██████████| 2/2 [00:00<00:00, 20.88it/s, loss=1.67, v_num=tune, train/objective=1.140, train/l2=1.130, train/sdl=81.30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_tune pid=17152)\u001b[0m \r",
      "Epoch 3: 100%|██████████| 2/2 [00:00<00:00, 12.31it/s, loss=1.67, v_num=tune, train/objective=1.140, train/l2=1.130, train/sdl=81.30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-10 12:20:14,181\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'activation': LeakyReLU(negative_slope=0.01)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m \r",
      "Epoch 0:  50%|█████     | 1/2 [00:59<00:59, 59.57s/it]\r",
      "Epoch 0:  50%|█████     | 1/2 [00:59<00:59, 59.57s/it]\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m \r",
      "Epoch 0:  50%|█████     | 1/2 [00:59<00:59, 59.57s/it, loss=1.97, v_num=tune, train/objective=1.970, train/l2=1.960, train/sdl=62.70]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-10 12:20:16,749\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'activation': Tanh()}\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m 2023-01-10 12:20:16.763087: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m 2023-01-10 12:20:16.763380: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_tune pid=8588)\u001b[0m \n",
      "Validation: 0it [00:00, ?it/s]\u001b[Am \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 2/2 [01:03<00:00, 31.83s/it, loss=1.97, v_num=tune, train/objective=1.970, train/l2=1.960, train/sdl=62.70]\n",
      "Epoch 0: 100%|██████████| 2/2 [01:03<00:00, 31.83s/it, loss=1.97, v_num=tune, train/objective=1.970, train/l2=1.960, train/sdl=62.70]\n",
      "Epoch 1:  50%|█████     | 1/2 [00:00<00:00, 31.13it/s, loss=1.65, v_num=tune, train/objective=1.330, train/l2=1.330, train/sdl=55.50]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[Am \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 2/2 [00:00<00:00, 31.32it/s, loss=1.65, v_num=tune, train/objective=1.330, train/l2=1.330, train/sdl=55.50]\n",
      "Epoch 1: 100%|██████████| 2/2 [00:00<00:00, 31.32it/s, loss=1.65, v_num=tune, train/objective=1.330, train/l2=1.330, train/sdl=55.50]\n",
      "Epoch 2:  50%|█████     | 1/2 [00:00<00:00, 32.01it/s, loss=1.44, v_num=tune, train/objective=1.030, train/l2=1.030, train/sdl=70.10]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[Am \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 2/2 [00:00<00:00, 31.25it/s, loss=1.44, v_num=tune, train/objective=1.030, train/l2=1.030, train/sdl=70.10]\n",
      "Epoch 3:   0%|          | 0/2 [00:00<?, ?it/s, loss=1.44, v_num=tune, train/objective=1.030, train/l2=1.030, train/sdl=70.10]        \n",
      "Epoch 3:  50%|█████     | 1/2 [00:00<00:00, 29.28it/s, loss=1.28, v_num=tune, train/objective=0.770, train/l2=0.761, train/sdl=81.70]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[Am \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 2/2 [00:00<00:00, 33.64it/s, loss=1.28, v_num=tune, train/objective=0.770, train/l2=0.761, train/sdl=81.70]\n",
      "Epoch 4:  50%|█████     | 1/2 [00:00<00:00, 31.94it/s, loss=1.15, v_num=tune, train/objective=0.664, train/l2=0.655, train/sdl=87.70]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[Am \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 2/2 [00:00<00:00, 63.88it/s, loss=1.15, v_num=tune, train/objective=0.664, train/l2=0.655, train/sdl=87.70]\n",
      "Epoch 4: 100%|██████████| 2/2 [00:00<00:00, 31.98it/s, loss=1.15, v_num=tune, train/objective=0.664, train/l2=0.655, train/sdl=87.70]\n",
      "Epoch 5:  50%|█████     | 1/2 [00:00<00:00, 62.19it/s, loss=1.07, v_num=tune, train/objective=0.626, train/l2=0.616, train/sdl=93.20]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[Am \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 2/2 [00:00<00:00, 42.24it/s, loss=1.07, v_num=tune, train/objective=0.626, train/l2=0.616, train/sdl=93.20]\n",
      "Epoch 6:   0%|          | 0/2 [00:00<?, ?it/s, loss=1.07, v_num=tune, train/objective=0.626, train/l2=0.616, train/sdl=93.20]        \n",
      "Epoch 6:  50%|█████     | 1/2 [00:00<00:00, 25.14it/s, loss=0.988, v_num=tune, train/objective=0.525, train/l2=0.516, train/sdl=93.50]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[Am \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 2/2 [00:00<00:00, 27.98it/s, loss=0.988, v_num=tune, train/objective=0.525, train/l2=0.516, train/sdl=93.50]\n",
      "Epoch 7:  50%|█████     | 1/2 [00:00<00:00, 31.98it/s, loss=0.927, v_num=tune, train/objective=0.497, train/l2=0.488, train/sdl=97.20]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[Am \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 2/2 [00:00<00:00, 63.96it/s, loss=0.927, v_num=tune, train/objective=0.497, train/l2=0.488, train/sdl=97.20]\n",
      "Epoch 7: 100%|██████████| 2/2 [00:00<00:00, 31.80it/s, loss=0.927, v_num=tune, train/objective=0.497, train/l2=0.488, train/sdl=97.20]\n",
      "Epoch 8:  50%|█████     | 1/2 [00:00<00:00, 32.01it/s, loss=0.869, v_num=tune, train/objective=0.406, train/l2=0.396, train/sdl=99.50]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[Am \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 2/2 [00:00<00:00, 32.02it/s, loss=0.869, v_num=tune, train/objective=0.406, train/l2=0.396, train/sdl=99.50]\n",
      "Epoch 9:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.869, v_num=tune, train/objective=0.406, train/l2=0.396, train/sdl=99.50]        \n",
      "Epoch 9:  50%|█████     | 1/2 [00:00<00:00, 32.83it/s, loss=0.819, v_num=tune, train/objective=0.368, train/l2=0.358, train/sdl=102.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[Am \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 2/2 [00:00<00:00, 31.98it/s, loss=0.819, v_num=tune, train/objective=0.368, train/l2=0.358, train/sdl=102.0]\n",
      "Epoch 9: 100%|██████████| 2/2 [00:00<00:00, 21.22it/s, loss=0.819, v_num=tune, train/objective=0.368, train/l2=0.358, train/sdl=102.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-10 12:20:24,429\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'activation': LeakyReLU(negative_slope=0.01)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m \r",
      "Epoch 0:  50%|█████     | 1/2 [00:59<00:59, 59.28s/it]\r",
      "Epoch 0:  50%|█████     | 1/2 [00:59<00:59, 59.28s/it]\r",
      "Epoch 0:  50%|█████     | 1/2 [00:59<00:59, 59.28s/it, loss=1.76, v_num=tune, train/objective=1.760, train/l2=1.750, train/sdl=56.00]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m 2023-01-10 12:20:27.955088: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m 2023-01-10 12:20:27.955402: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 2/2 [01:02<00:00, 31.38s/it, loss=1.76, v_num=tune, train/objective=1.760, train/l2=1.750, train/sdl=56.00]\n",
      "Epoch 0: 100%|██████████| 2/2 [01:02<00:00, 31.40s/it, loss=1.76, v_num=tune, train/objective=1.760, train/l2=1.750, train/sdl=56.00]\n",
      "Epoch 1:   0%|          | 0/2 [00:00<?, ?it/s, loss=1.76, v_num=tune, train/objective=1.760, train/l2=1.750, train/sdl=56.00]        \n",
      "Epoch 1:  50%|█████     | 1/2 [00:00<00:00, 22.25it/s, loss=1.88, v_num=tune, train/objective=2.000, train/l2=1.990, train/sdl=82.40]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "\u001b[2m\u001b[36m(train_tune pid=10572)\u001b[0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 2/2 [00:00<00:00, 35.08it/s, loss=1.88, v_num=tune, train/objective=2.000, train/l2=1.990, train/sdl=82.40]\n",
      "Epoch 1: 100%|██████████| 2/2 [00:00<00:00, 26.08it/s, loss=1.88, v_num=tune, train/objective=2.000, train/l2=1.990, train/sdl=82.40]\n",
      "Epoch 2:  50%|█████     | 1/2 [00:00<00:00, 27.37it/s, loss=1.64, v_num=tune, train/objective=1.180, train/l2=1.170, train/sdl=102.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 2/2 [00:00<00:00, 44.92it/s, loss=1.64, v_num=tune, train/objective=1.180, train/l2=1.170, train/sdl=102.0]\n",
      "Epoch 2: 100%|██████████| 2/2 [00:00<00:00, 32.98it/s, loss=1.64, v_num=tune, train/objective=1.180, train/l2=1.170, train/sdl=102.0]\n",
      "Epoch 2: 100%|██████████| 2/2 [00:00<00:00, 32.98it/s, loss=1.64, v_num=tune, train/objective=1.180, train/l2=1.170, train/sdl=102.0]\n",
      "Epoch 3:  50%|█████     | 1/2 [00:00<00:00, 25.36it/s, loss=1.47, v_num=tune, train/objective=0.953, train/l2=0.941, train/sdl=122.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 2/2 [00:00<00:00, 31.52it/s, loss=1.47, v_num=tune, train/objective=0.953, train/l2=0.941, train/sdl=122.0]\n",
      "                                                              \u001b[A\n",
      "Epoch 3: 100%|██████████| 2/2 [00:00<00:00, 27.98it/s, loss=1.47, v_num=tune, train/objective=0.953, train/l2=0.941, train/sdl=122.0]\n",
      "Epoch 4:  50%|█████     | 1/2 [00:00<00:00, 34.10it/s, loss=1.37, v_num=tune, train/objective=0.953, train/l2=0.940, train/sdl=121.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 2/2 [00:00<00:00, 44.13it/s, loss=1.37, v_num=tune, train/objective=0.953, train/l2=0.940, train/sdl=121.0]\n",
      "Epoch 4: 100%|██████████| 2/2 [00:00<00:00, 32.62it/s, loss=1.37, v_num=tune, train/objective=0.953, train/l2=0.940, train/sdl=121.0]\n",
      "Epoch 4: 100%|██████████| 2/2 [00:00<00:00, 32.62it/s, loss=1.37, v_num=tune, train/objective=0.953, train/l2=0.940, train/sdl=121.0]\n",
      "Epoch 5:  50%|█████     | 1/2 [00:00<00:00, 31.47it/s, loss=1.26, v_num=tune, train/objective=0.704, train/l2=0.691, train/sdl=130.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 2/2 [00:00<00:00, 32.45it/s, loss=1.26, v_num=tune, train/objective=0.704, train/l2=0.691, train/sdl=130.0]\n",
      "Epoch 5: 100%|██████████| 2/2 [00:00<00:00, 32.45it/s, loss=1.26, v_num=tune, train/objective=0.704, train/l2=0.691, train/sdl=130.0]\n",
      "Epoch 6:  50%|█████     | 1/2 [00:00<00:00, 32.01it/s, loss=1.17, v_num=tune, train/objective=0.639, train/l2=0.625, train/sdl=134.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 2/2 [00:00<00:00, 36.06it/s, loss=1.17, v_num=tune, train/objective=0.639, train/l2=0.625, train/sdl=134.0]\n",
      "Epoch 6: 100%|██████████| 2/2 [00:00<00:00, 36.06it/s, loss=1.17, v_num=tune, train/objective=0.639, train/l2=0.625, train/sdl=134.0]\n",
      "Epoch 7:  50%|█████     | 1/2 [00:00<00:00, 63.94it/s, loss=1.1, v_num=tune, train/objective=0.586, train/l2=0.572, train/sdl=135.0] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 2/2 [00:00<00:00, 33.48it/s, loss=1.1, v_num=tune, train/objective=0.586, train/l2=0.572, train/sdl=135.0]\n",
      "Epoch 7: 100%|██████████| 2/2 [00:00<00:00, 33.48it/s, loss=1.1, v_num=tune, train/objective=0.586, train/l2=0.572, train/sdl=135.0]\n",
      "Epoch 8:  50%|█████     | 1/2 [00:00<00:00, 32.01it/s, loss=1.03, v_num=tune, train/objective=0.543, train/l2=0.530, train/sdl=135.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 2/2 [00:00<00:00, 31.92it/s, loss=1.03, v_num=tune, train/objective=0.543, train/l2=0.530, train/sdl=135.0]\n",
      "Epoch 8: 100%|██████████| 2/2 [00:00<00:00, 31.92it/s, loss=1.03, v_num=tune, train/objective=0.543, train/l2=0.530, train/sdl=135.0]\n",
      "Epoch 9:  50%|█████     | 1/2 [00:00<00:00, 31.99it/s, loss=0.982, v_num=tune, train/objective=0.512, train/l2=0.499, train/sdl=135.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 2/2 [00:00<00:00, 31.79it/s, loss=0.982, v_num=tune, train/objective=0.512, train/l2=0.499, train/sdl=135.0]\n",
      "Epoch 9: 100%|██████████| 2/2 [00:00<00:00, 31.79it/s, loss=0.982, v_num=tune, train/objective=0.512, train/l2=0.499, train/sdl=135.0]\n",
      "Epoch 9: 100%|██████████| 2/2 [00:00<00:00, 15.81it/s, loss=0.982, v_num=tune, train/objective=0.512, train/l2=0.499, train/sdl=135.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-10 12:20:35,086\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'activation': ReLU()}\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m 2023-01-10 12:20:37.494756: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m 2023-01-10 12:20:37.494836: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  50%|█████     | 1/2 [00:51<00:51, 51.69s/it, loss=1.9, v_num=tune, train/objective=1.900, train/l2=1.900, train/sdl=62.40]\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 2/2 [00:55<00:00, 27.75s/it, loss=1.9, v_num=tune, train/objective=1.900, train/l2=1.900, train/sdl=62.40]\n",
      "Epoch 0: 100%|██████████| 2/2 [00:55<00:00, 27.78s/it, loss=1.9, v_num=tune, train/objective=1.900, train/l2=1.900, train/sdl=62.40]\n",
      "Epoch 0: 100%|██████████| 2/2 [00:55<00:00, 27.78s/it, loss=1.9, v_num=tune, train/objective=1.900, train/l2=1.900, train/sdl=62.40]\n",
      "Epoch 1:  50%|█████     | 1/2 [00:00<00:00, 34.79it/s, loss=1.91, v_num=tune, train/objective=1.930, train/l2=1.920, train/sdl=81.40]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 2/2 [00:00<00:00, 33.36it/s, loss=1.91, v_num=tune, train/objective=1.930, train/l2=1.920, train/sdl=81.40]\n",
      "Epoch 1: 100%|██████████| 2/2 [00:00<00:00, 26.47it/s, loss=1.91, v_num=tune, train/objective=1.930, train/l2=1.920, train/sdl=81.40]\n",
      "Epoch 2:  50%|█████     | 1/2 [00:00<00:00, 23.20it/s, loss=1.76, v_num=tune, train/objective=1.440, train/l2=1.430, train/sdl=95.80]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 2/2 [00:00<00:00, 26.68it/s, loss=1.76, v_num=tune, train/objective=1.440, train/l2=1.430, train/sdl=95.80]\n",
      "Epoch 2: 100%|██████████| 2/2 [00:00<00:00, 26.68it/s, loss=1.76, v_num=tune, train/objective=1.440, train/l2=1.430, train/sdl=95.80]\n",
      "Epoch 3:  50%|█████     | 1/2 [00:00<00:00, 29.56it/s, loss=1.59, v_num=tune, train/objective=1.110, train/l2=1.100, train/sdl=112.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 2/2 [00:00<00:00, 42.22it/s, loss=1.59, v_num=tune, train/objective=1.110, train/l2=1.100, train/sdl=112.0]\n",
      "Epoch 3: 100%|██████████| 2/2 [00:00<00:00, 31.73it/s, loss=1.59, v_num=tune, train/objective=1.110, train/l2=1.100, train/sdl=112.0]\n",
      "Epoch 4:   0%|          | 0/2 [00:00<?, ?it/s, loss=1.59, v_num=tune, train/objective=1.110, train/l2=1.100, train/sdl=112.0]        \n",
      "Epoch 4:  50%|█████     | 1/2 [00:00<00:00, 25.58it/s, loss=1.44, v_num=tune, train/objective=0.837, train/l2=0.825, train/sdl=121.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 2/2 [00:00<00:00, 42.25it/s, loss=1.44, v_num=tune, train/objective=0.837, train/l2=0.825, train/sdl=121.0]\n",
      "Epoch 4: 100%|██████████| 2/2 [00:00<00:00, 25.44it/s, loss=1.44, v_num=tune, train/objective=0.837, train/l2=0.825, train/sdl=121.0]\n",
      "Epoch 5:   0%|          | 0/2 [00:00<?, ?it/s, loss=1.44, v_num=tune, train/objective=0.837, train/l2=0.825, train/sdl=121.0]        \n",
      "Epoch 5:  50%|█████     | 1/2 [00:00<00:00, 31.96it/s, loss=1.36, v_num=tune, train/objective=0.939, train/l2=0.927, train/sdl=118.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 2/2 [00:00<00:00, 38.90it/s, loss=1.36, v_num=tune, train/objective=0.939, train/l2=0.927, train/sdl=118.0]\n",
      "Epoch 5: 100%|██████████| 2/2 [00:00<00:00, 31.78it/s, loss=1.36, v_num=tune, train/objective=0.939, train/l2=0.927, train/sdl=118.0]\n",
      "Epoch 6:  50%|█████     | 1/2 [00:00<00:00, 26.76it/s, loss=1.26, v_num=tune, train/objective=0.680, train/l2=0.667, train/sdl=123.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 2/2 [00:00<00:00, 39.49it/s, loss=1.26, v_num=tune, train/objective=0.680, train/l2=0.667, train/sdl=123.0]\n",
      "Epoch 6: 100%|██████████| 2/2 [00:00<00:00, 30.01it/s, loss=1.26, v_num=tune, train/objective=0.680, train/l2=0.667, train/sdl=123.0]\n",
      "Epoch 7:  50%|█████     | 1/2 [00:00<00:00, 31.93it/s, loss=1.26, v_num=tune, train/objective=0.680, train/l2=0.667, train/sdl=123.0]\n",
      "Epoch 7:  50%|█████     | 1/2 [00:00<00:00, 31.93it/s, loss=1.18, v_num=tune, train/objective=0.603, train/l2=0.590, train/sdl=126.0]\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(train_tune pid=16212)\u001b[0m \n",
      "Epoch 7: 100%|██████████| 2/2 [00:00<00:00, 42.64it/s, loss=1.18, v_num=tune, train/objective=0.603, train/l2=0.590, train/sdl=126.0]\n",
      "Epoch 7: 100%|██████████| 2/2 [00:00<00:00, 31.98it/s, loss=1.18, v_num=tune, train/objective=0.603, train/l2=0.590, train/sdl=126.0]\n",
      "                                                              \u001b[A\n",
      "Epoch 7: 100%|██████████| 2/2 [00:00<00:00, 25.59it/s, loss=1.18, v_num=tune, train/objective=0.603, train/l2=0.590, train/sdl=126.0]\n",
      "Epoch 8:  50%|█████     | 1/2 [00:00<00:00, 30.71it/s, loss=1.11, v_num=tune, train/objective=0.541, train/l2=0.528, train/sdl=125.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 2/2 [00:00<00:00, 41.22it/s, loss=1.11, v_num=tune, train/objective=0.541, train/l2=0.528, train/sdl=125.0]\n",
      "Epoch 8: 100%|██████████| 2/2 [00:00<00:00, 27.70it/s, loss=1.11, v_num=tune, train/objective=0.541, train/l2=0.528, train/sdl=125.0]\n",
      "Epoch 8: 100%|██████████| 2/2 [00:00<00:00, 27.70it/s, loss=1.11, v_num=tune, train/objective=0.541, train/l2=0.528, train/sdl=125.0]\n",
      "Epoch 9:  50%|█████     | 1/2 [00:00<00:00, 21.07it/s, loss=1.05, v_num=tune, train/objective=0.483, train/l2=0.471, train/sdl=125.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 2/2 [00:00<00:00, 31.73it/s, loss=1.05, v_num=tune, train/objective=0.483, train/l2=0.471, train/sdl=125.0]\n",
      "Epoch 9: 100%|██████████| 2/2 [00:00<00:00, 25.43it/s, loss=1.05, v_num=tune, train/objective=0.483, train/l2=0.471, train/sdl=125.0]\n",
      "Epoch 9: 100%|██████████| 2/2 [00:00<00:00, 12.12it/s, loss=1.05, v_num=tune, train/objective=0.483, train/l2=0.471, train/sdl=125.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-10 12:20:44,959\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'activation': LeakyReLU(negative_slope=0.01)}\n",
      "2023-01-10 12:20:45,116\tINFO tune.py:762 -- Total run time: 134.43 seconds (134.16 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "tuner = tune.Tuner(\n",
    "    tune.with_resources(train_tune, {\"cpu\": 1, \"extra_cpu\": 4}),\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric='loss', # key name of the metrics dict\n",
    "        mode=\"min\",\n",
    "        num_samples=5,\n",
    "    ),\n",
    "    param_space=config\n",
    ")\n",
    "\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d4f9b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'optimizer': 'sgd',\n",
       " 'lr': 0.09899886332345004,\n",
       " 'batch_size': 384,\n",
       " 'latent_dims': 30,\n",
       " 'dropout': 0.1,\n",
       " 'activation': LeakyReLU(negative_slope=0.01)}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.get_best_result().config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b16ebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#another way of doing the same step above\n",
    "\n",
    "# from functools import partial\n",
    "# sdl_ray = tune.run(\n",
    "#  partial(train_tune,gpus=4),config=config, num_samples=5, metric=\"val/l2\", mode='min'\n",
    "#  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3586c365",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sdl_ray.get_best_config(\"loss\", 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f759e5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### END of ray-tune-test ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4af14604",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Files:\n",
    "    def __init__(self, filename):\n",
    "        self.file = filename\n",
    "        \n",
    "    def write_to_file(self, data):\n",
    "        with open(self.file, 'wb') as f:\n",
    "            pickle.dump(data, f) \n",
    "        return None\n",
    "    \n",
    "    def load_pickle(self):\n",
    "        data = pd.read_pickle(self.file)\n",
    "        return data\n",
    "    \n",
    "    def load_csv(self, sep, usecols=None):\n",
    "        data = pd.read_csv(self.file, sep=sep, usecols=usecols)\n",
    "        return data\n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12750a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tanis = Files('./GNPS_15_12_2021_pos_tanimoto_scores.pickle').load_pickle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45d31c26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>inchikey14</th>\n",
       "      <th>LFTLOKWAGJYHHR</th>\n",
       "      <th>BQDXDGDOYPUUOD</th>\n",
       "      <th>VEPUCZUJLKAVNM</th>\n",
       "      <th>PXPSEALQIQRPQY</th>\n",
       "      <th>HDZVRBPBPCZCJG</th>\n",
       "      <th>SXJIZQPZESTWLD</th>\n",
       "      <th>JDOFZOKGCYYUER</th>\n",
       "      <th>WGTCMJBJRPKENJ</th>\n",
       "      <th>FCCDDURTIIUXBY</th>\n",
       "      <th>FDLLEBFMOIHMNM</th>\n",
       "      <th>...</th>\n",
       "      <th>RJAHLSXSRQXGGI</th>\n",
       "      <th>VKJTXCWIQDBMLE</th>\n",
       "      <th>NFIHKFSODJJLGC</th>\n",
       "      <th>NHLBOKNHQIEJIH</th>\n",
       "      <th>QABASLXUKXNHMC</th>\n",
       "      <th>XGVJWXAYKUHDOO</th>\n",
       "      <th>MNKNQKOOKLVXDB</th>\n",
       "      <th>CQKNELOTFUSOTP</th>\n",
       "      <th>MHCYVCDXRQGUFW</th>\n",
       "      <th>NMCMVEXMLSARCJ</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inchikey14</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LFTLOKWAGJYHHR</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.057353</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.053269</td>\n",
       "      <td>0.069264</td>\n",
       "      <td>0.055453</td>\n",
       "      <td>0.048193</td>\n",
       "      <td>0.053296</td>\n",
       "      <td>0.052863</td>\n",
       "      <td>0.056204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049612</td>\n",
       "      <td>0.054762</td>\n",
       "      <td>0.053929</td>\n",
       "      <td>0.060065</td>\n",
       "      <td>0.049683</td>\n",
       "      <td>0.052980</td>\n",
       "      <td>0.049046</td>\n",
       "      <td>0.095833</td>\n",
       "      <td>0.050964</td>\n",
       "      <td>0.050159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BQDXDGDOYPUUOD</th>\n",
       "      <td>0.057353</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.162866</td>\n",
       "      <td>0.215026</td>\n",
       "      <td>0.242169</td>\n",
       "      <td>0.176221</td>\n",
       "      <td>0.296270</td>\n",
       "      <td>0.195915</td>\n",
       "      <td>0.089888</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185547</td>\n",
       "      <td>0.430151</td>\n",
       "      <td>0.180851</td>\n",
       "      <td>0.218014</td>\n",
       "      <td>0.321244</td>\n",
       "      <td>0.297297</td>\n",
       "      <td>0.272672</td>\n",
       "      <td>0.147776</td>\n",
       "      <td>0.317369</td>\n",
       "      <td>0.253207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VEPUCZUJLKAVNM</th>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.162866</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.286316</td>\n",
       "      <td>0.113158</td>\n",
       "      <td>0.152310</td>\n",
       "      <td>0.208607</td>\n",
       "      <td>0.264908</td>\n",
       "      <td>0.082418</td>\n",
       "      <td>0.186620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.157480</td>\n",
       "      <td>0.167925</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.152523</td>\n",
       "      <td>0.228311</td>\n",
       "      <td>0.251228</td>\n",
       "      <td>0.226978</td>\n",
       "      <td>0.080844</td>\n",
       "      <td>0.289835</td>\n",
       "      <td>0.231393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PXPSEALQIQRPQY</th>\n",
       "      <td>0.053269</td>\n",
       "      <td>0.215026</td>\n",
       "      <td>0.286316</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.134357</td>\n",
       "      <td>0.185499</td>\n",
       "      <td>0.342992</td>\n",
       "      <td>0.344860</td>\n",
       "      <td>0.081680</td>\n",
       "      <td>0.257655</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243169</td>\n",
       "      <td>0.248216</td>\n",
       "      <td>0.351724</td>\n",
       "      <td>0.207021</td>\n",
       "      <td>0.299073</td>\n",
       "      <td>0.376868</td>\n",
       "      <td>0.351122</td>\n",
       "      <td>0.073363</td>\n",
       "      <td>0.362462</td>\n",
       "      <td>0.374658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HDZVRBPBPCZCJG</th>\n",
       "      <td>0.069264</td>\n",
       "      <td>0.242169</td>\n",
       "      <td>0.113158</td>\n",
       "      <td>0.134357</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.145266</td>\n",
       "      <td>0.205817</td>\n",
       "      <td>0.136898</td>\n",
       "      <td>0.102000</td>\n",
       "      <td>0.240987</td>\n",
       "      <td>...</td>\n",
       "      <td>0.139401</td>\n",
       "      <td>0.275825</td>\n",
       "      <td>0.136986</td>\n",
       "      <td>0.157074</td>\n",
       "      <td>0.223055</td>\n",
       "      <td>0.192202</td>\n",
       "      <td>0.190231</td>\n",
       "      <td>0.166329</td>\n",
       "      <td>0.196615</td>\n",
       "      <td>0.175919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 20889 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "inchikey14      LFTLOKWAGJYHHR  BQDXDGDOYPUUOD  VEPUCZUJLKAVNM  \\\n",
       "inchikey14                                                       \n",
       "LFTLOKWAGJYHHR        1.000000        0.057353        0.042969   \n",
       "BQDXDGDOYPUUOD        0.057353        1.000000        0.162866   \n",
       "VEPUCZUJLKAVNM        0.042969        0.162866        1.000000   \n",
       "PXPSEALQIQRPQY        0.053269        0.215026        0.286316   \n",
       "HDZVRBPBPCZCJG        0.069264        0.242169        0.113158   \n",
       "\n",
       "inchikey14      PXPSEALQIQRPQY  HDZVRBPBPCZCJG  SXJIZQPZESTWLD  \\\n",
       "inchikey14                                                       \n",
       "LFTLOKWAGJYHHR        0.053269        0.069264        0.055453   \n",
       "BQDXDGDOYPUUOD        0.215026        0.242169        0.176221   \n",
       "VEPUCZUJLKAVNM        0.286316        0.113158        0.152310   \n",
       "PXPSEALQIQRPQY        1.000000        0.134357        0.185499   \n",
       "HDZVRBPBPCZCJG        0.134357        1.000000        0.145266   \n",
       "\n",
       "inchikey14      JDOFZOKGCYYUER  WGTCMJBJRPKENJ  FCCDDURTIIUXBY  \\\n",
       "inchikey14                                                       \n",
       "LFTLOKWAGJYHHR        0.048193        0.053296        0.052863   \n",
       "BQDXDGDOYPUUOD        0.296270        0.195915        0.089888   \n",
       "VEPUCZUJLKAVNM        0.208607        0.264908        0.082418   \n",
       "PXPSEALQIQRPQY        0.342992        0.344860        0.081680   \n",
       "HDZVRBPBPCZCJG        0.205817        0.136898        0.102000   \n",
       "\n",
       "inchikey14      FDLLEBFMOIHMNM  ...  RJAHLSXSRQXGGI  VKJTXCWIQDBMLE  \\\n",
       "inchikey14                      ...                                   \n",
       "LFTLOKWAGJYHHR        0.056204  ...        0.049612        0.054762   \n",
       "BQDXDGDOYPUUOD        0.460000  ...        0.185547        0.430151   \n",
       "VEPUCZUJLKAVNM        0.186620  ...        0.157480        0.167925   \n",
       "PXPSEALQIQRPQY        0.257655  ...        0.243169        0.248216   \n",
       "HDZVRBPBPCZCJG        0.240987  ...        0.139401        0.275825   \n",
       "\n",
       "inchikey14      NFIHKFSODJJLGC  NHLBOKNHQIEJIH  QABASLXUKXNHMC  \\\n",
       "inchikey14                                                       \n",
       "LFTLOKWAGJYHHR        0.053929        0.060065        0.049683   \n",
       "BQDXDGDOYPUUOD        0.180851        0.218014        0.321244   \n",
       "VEPUCZUJLKAVNM        0.233333        0.152523        0.228311   \n",
       "PXPSEALQIQRPQY        0.351724        0.207021        0.299073   \n",
       "HDZVRBPBPCZCJG        0.136986        0.157074        0.223055   \n",
       "\n",
       "inchikey14      XGVJWXAYKUHDOO  MNKNQKOOKLVXDB  CQKNELOTFUSOTP  \\\n",
       "inchikey14                                                       \n",
       "LFTLOKWAGJYHHR        0.052980        0.049046        0.095833   \n",
       "BQDXDGDOYPUUOD        0.297297        0.272672        0.147776   \n",
       "VEPUCZUJLKAVNM        0.251228        0.226978        0.080844   \n",
       "PXPSEALQIQRPQY        0.376868        0.351122        0.073363   \n",
       "HDZVRBPBPCZCJG        0.192202        0.190231        0.166329   \n",
       "\n",
       "inchikey14      MHCYVCDXRQGUFW  NMCMVEXMLSARCJ  \n",
       "inchikey14                                      \n",
       "LFTLOKWAGJYHHR        0.050964        0.050159  \n",
       "BQDXDGDOYPUUOD        0.317369        0.253207  \n",
       "VEPUCZUJLKAVNM        0.289835        0.231393  \n",
       "PXPSEALQIQRPQY        0.362462        0.374658  \n",
       "HDZVRBPBPCZCJG        0.196615        0.175919  \n",
       "\n",
       "[5 rows x 20889 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46d5437d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04296875"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanis.loc['LFTLOKWAGJYHHR', 'VEPUCZUJLKAVNM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ebf2f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>inchikey14</th>\n",
       "      <th>LFTLOKWAGJYHHR</th>\n",
       "      <th>BQDXDGDOYPUUOD</th>\n",
       "      <th>VEPUCZUJLKAVNM</th>\n",
       "      <th>PXPSEALQIQRPQY</th>\n",
       "      <th>HDZVRBPBPCZCJG</th>\n",
       "      <th>SXJIZQPZESTWLD</th>\n",
       "      <th>JDOFZOKGCYYUER</th>\n",
       "      <th>WGTCMJBJRPKENJ</th>\n",
       "      <th>FCCDDURTIIUXBY</th>\n",
       "      <th>FDLLEBFMOIHMNM</th>\n",
       "      <th>...</th>\n",
       "      <th>RJAHLSXSRQXGGI</th>\n",
       "      <th>VKJTXCWIQDBMLE</th>\n",
       "      <th>NFIHKFSODJJLGC</th>\n",
       "      <th>NHLBOKNHQIEJIH</th>\n",
       "      <th>QABASLXUKXNHMC</th>\n",
       "      <th>XGVJWXAYKUHDOO</th>\n",
       "      <th>MNKNQKOOKLVXDB</th>\n",
       "      <th>CQKNELOTFUSOTP</th>\n",
       "      <th>MHCYVCDXRQGUFW</th>\n",
       "      <th>NMCMVEXMLSARCJ</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inchikey14</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LFTLOKWAGJYHHR</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BQDXDGDOYPUUOD</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VEPUCZUJLKAVNM</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PXPSEALQIQRPQY</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HDZVRBPBPCZCJG</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 20889 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "inchikey14      LFTLOKWAGJYHHR  BQDXDGDOYPUUOD  VEPUCZUJLKAVNM  \\\n",
       "inchikey14                                                       \n",
       "LFTLOKWAGJYHHR             1.0             NaN             NaN   \n",
       "BQDXDGDOYPUUOD             NaN             1.0             NaN   \n",
       "VEPUCZUJLKAVNM             NaN             NaN             1.0   \n",
       "PXPSEALQIQRPQY             NaN             NaN             NaN   \n",
       "HDZVRBPBPCZCJG             NaN             NaN             NaN   \n",
       "\n",
       "inchikey14      PXPSEALQIQRPQY  HDZVRBPBPCZCJG  SXJIZQPZESTWLD  \\\n",
       "inchikey14                                                       \n",
       "LFTLOKWAGJYHHR             NaN             NaN             NaN   \n",
       "BQDXDGDOYPUUOD             NaN             NaN             NaN   \n",
       "VEPUCZUJLKAVNM             NaN             NaN             NaN   \n",
       "PXPSEALQIQRPQY             1.0             NaN             NaN   \n",
       "HDZVRBPBPCZCJG             NaN             1.0             NaN   \n",
       "\n",
       "inchikey14      JDOFZOKGCYYUER  WGTCMJBJRPKENJ  FCCDDURTIIUXBY  \\\n",
       "inchikey14                                                       \n",
       "LFTLOKWAGJYHHR             NaN             NaN             NaN   \n",
       "BQDXDGDOYPUUOD             NaN             NaN             NaN   \n",
       "VEPUCZUJLKAVNM             NaN             NaN             NaN   \n",
       "PXPSEALQIQRPQY             NaN             NaN             NaN   \n",
       "HDZVRBPBPCZCJG             NaN             NaN             NaN   \n",
       "\n",
       "inchikey14      FDLLEBFMOIHMNM  ...  RJAHLSXSRQXGGI  VKJTXCWIQDBMLE  \\\n",
       "inchikey14                      ...                                   \n",
       "LFTLOKWAGJYHHR             NaN  ...             NaN             NaN   \n",
       "BQDXDGDOYPUUOD             NaN  ...             NaN             NaN   \n",
       "VEPUCZUJLKAVNM             NaN  ...             NaN             NaN   \n",
       "PXPSEALQIQRPQY             NaN  ...             NaN             NaN   \n",
       "HDZVRBPBPCZCJG             NaN  ...             NaN             NaN   \n",
       "\n",
       "inchikey14      NFIHKFSODJJLGC  NHLBOKNHQIEJIH  QABASLXUKXNHMC  \\\n",
       "inchikey14                                                       \n",
       "LFTLOKWAGJYHHR             NaN             NaN             NaN   \n",
       "BQDXDGDOYPUUOD             NaN             NaN             NaN   \n",
       "VEPUCZUJLKAVNM             NaN             NaN             NaN   \n",
       "PXPSEALQIQRPQY             NaN             NaN             NaN   \n",
       "HDZVRBPBPCZCJG             NaN             NaN             NaN   \n",
       "\n",
       "inchikey14      XGVJWXAYKUHDOO  MNKNQKOOKLVXDB  CQKNELOTFUSOTP  \\\n",
       "inchikey14                                                       \n",
       "LFTLOKWAGJYHHR             NaN             NaN             NaN   \n",
       "BQDXDGDOYPUUOD             NaN             NaN             NaN   \n",
       "VEPUCZUJLKAVNM             NaN             NaN             NaN   \n",
       "PXPSEALQIQRPQY             NaN             NaN             NaN   \n",
       "HDZVRBPBPCZCJG             NaN             NaN             NaN   \n",
       "\n",
       "inchikey14      MHCYVCDXRQGUFW  NMCMVEXMLSARCJ  \n",
       "inchikey14                                      \n",
       "LFTLOKWAGJYHHR             NaN             NaN  \n",
       "BQDXDGDOYPUUOD             NaN             NaN  \n",
       "VEPUCZUJLKAVNM             NaN             NaN  \n",
       "PXPSEALQIQRPQY             NaN             NaN  \n",
       "HDZVRBPBPCZCJG             NaN             NaN  \n",
       "\n",
       "[5 rows x 20889 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7de2e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
