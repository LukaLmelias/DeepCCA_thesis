{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ded05db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from cca_zoo.deepmodels import (\n",
    "    DCCA,\n",
    "    DCCA_NOI,\n",
    "    DCCA_SDL,\n",
    "    #BarlowTwins,\n",
    "    get_dataloaders,\n",
    "    \n",
    ")\n",
    "from cca_zoo.deepmodels.utils import architectures, objectives\n",
    "from cca_zoo.plotting import pairplot_label\n",
    "from cca_zoo.data import CCA_Dataset\n",
    "from cca_zoo.models import CCA\n",
    "from cca_zoo.model_selection import GridSearchCV\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import fisher_exact\n",
    "from scipy.stats.contingency import crosstab\n",
    "from scipy.stats import hypergeom\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch import nn\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "from ray.tune.integration.pytorch_lightning import tune\n",
    "os.chdir('../raw_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b5dcf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Files:\n",
    "    def __init__(self, filename):\n",
    "        self.file = filename\n",
    "        \n",
    "    def write_to_file(self, data):\n",
    "        with open(self.file, 'wb') as f:\n",
    "            pickle.dump(data, f) \n",
    "        return None\n",
    "    \n",
    "    def load_pickle(self):\n",
    "        data = pd.read_pickle(self.file)\n",
    "        return data\n",
    "    \n",
    "    def load_csv(self, sep, usecols=None):\n",
    "        data = pd.read_csv(self.file, sep=sep, usecols=usecols)\n",
    "        return data\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f475c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 8)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load df with all info\n",
    "path = './df_classes_max3_embeddings.pickle'\n",
    "df_all = Files(path).load_pickle()[:100]#just a subset for scripting\n",
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0e52552",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCCA:\n",
    "    def __init__(self, df_all,batch_size = 768,num_workers = 6,\\\n",
    "                latent_dims=100, epochs=300, lr=0.001): #default dims determined after iteratin 10:50 dims \n",
    "        \n",
    "        self.df_all = df_all\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.v1='ms2ds'\n",
    "        self.v2 = 'mol2vec'\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        self.sdl_lr = 0.025118864315095822#0.01#lr (picked after running lr_finder)\n",
    "        self.dcca_lr = 5.623413251903491e-08 #lr\n",
    "        self.latent_dims=latent_dims\n",
    "        self.optim = 'sgd'\n",
    "        self.activation = nn.Tanh()\n",
    "        self.objective = objectives.CCA\n",
    "        self.encoder_1_layers = (500,500)\n",
    "        self.encoder_2_layers = (500,500)\n",
    "        seed_everything(15)\n",
    "        \n",
    "        \n",
    "    def split_data(self,test_size=0.2,\\\n",
    "                   random_state=None,stratify=None): # thinking of removing this one\n",
    "        \n",
    "        if random_state != None and stratify == None:\n",
    "            train_df, test_df = \\\n",
    "            train_test_split(self.df_all, test_size=test_size, random_state=random_state)\n",
    "        \n",
    "        elif random_state == None and stratify != None:\n",
    "            train_df, test_df = \\\n",
    "            train_test_split(self.df_all, test_size=test_size,stratify=self.df_all[stratify])\n",
    "        else:\n",
    "            train_df, test_df = \\\n",
    "            train_test_split(self.df_all, test_size=test_size, random_state=42)\n",
    "        \n",
    "        return train_df, test_df \n",
    "    \n",
    "    def gen_views(self,v1='ms2ds',v2='mol2vec'):\n",
    "        \n",
    "        #split test, train\n",
    "        train_df, test_df= self.split_data(test_size=0.2,random_state=42)\n",
    "        \n",
    "        #Split train dataset into train and validation set\n",
    "        train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "        \n",
    "        \n",
    "        #extract the 2 view, v1 == spectra embeddings, v2==structure embeddings\n",
    "        v1_train, v1_test = np.array([x for x in train_df[v1]]), np.array([x for x in test_df[v1]])\n",
    "        v2_train, v2_test = np.array([x for x in train_df[v2]]), np.array([x for x in test_df[v2]])\n",
    "\n",
    "        # validation\n",
    "        v1_val, v2_val = np.array([x for x in val_df[v1]]), np.array([x for x in val_df[v2]])\n",
    "        \n",
    "        #update self dfs\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.val_df = val_df\n",
    "        \n",
    "        # not memory efficient !!!\n",
    "        self.v1_train, self.v1_test = v1_train, v1_test\n",
    "        self.v2_train, self.v2_test = v2_train, v2_test\n",
    "        self.v1_val, self.v2_val = v1_val, v2_val\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def deepcca_encoders(self):\n",
    "        # define encoders\n",
    "        self.encoder_1 = architectures.Encoder(\n",
    "                                      latent_dims = self.latent_dims, \n",
    "                                      feature_size = self.v1_size,\n",
    "                                      layer_sizes = self.encoder_1_layers,\n",
    "                                      activation = self.activation\n",
    "                                    )\n",
    "                                     \n",
    "        self.encoder_2 = architectures.Encoder(\n",
    "                                      latent_dims=self.latent_dims, \n",
    "                                      feature_size=self.v2_size, \n",
    "                                      layer_sizes=self.encoder_2_layers,\n",
    "                                      activation = self.activation\n",
    "                                     )\n",
    "       \n",
    "        return None#[encoder_1, encoder_2]\n",
    "        \n",
    "    \n",
    "    def deepcca_dataloaders(self):\n",
    "        \n",
    "        #v1_train,v1_test, v2_train,v2_test, v1_val, v2_val = \\\n",
    "        self.gen_views(v1=self.v1, v2=self.v2)\n",
    "        \n",
    "        #creat CCA dataset \n",
    "        train_dataset = CCA_Dataset([self.v1_train, self.v2_train])\n",
    "        test_dataset = CCA_Dataset([self.v1_test, self.v2_test])\n",
    "        val_dataset = CCA_Dataset([self.v1_val, self.v2_val])\n",
    "        \n",
    "        #update features size\n",
    "        self.v1_size = self.v1_train.shape[1]\n",
    "        self.v2_size = self.v2_train.shape[1]\n",
    "        self.N = len(train_dataset)\n",
    "        \n",
    "        #set N (for sdl; equal len train dataset)\n",
    "        self.N = len(train_dataset)\n",
    "        \n",
    "        #loaders\n",
    "        self.train_loader , self.val_loader = get_dataloaders(train_dataset, \\\n",
    "                                                    val_dataset,batch_size=self.batch_size,\\\n",
    "                                                    num_workers=self.num_workers,drop_last=False)\n",
    "        self.test_loader = get_dataloaders(test_dataset,\\\n",
    "                                      batch_size=self.batch_size, \\\n",
    "                                      num_workers=self.num_workers,drop_last=False)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def train_cca(self): #only for comparison with other deep models\n",
    "        \n",
    "       \n",
    "        \n",
    "        #define and train cca model\n",
    "        print('\\n','-'*20,'\\n Training CCA\\n','-'*20)\n",
    "        cca = CCA(latent_dims=self.latent_dims).fit((self.v1_train, self.v2_train))\n",
    "        self.cca = cca\n",
    "        return None#cca\n",
    "    \n",
    "    def train_sdl(self, checkpoint=None, logger=None,lam=0.0001,enable_progress_bar=True ):\n",
    "        \n",
    "        \n",
    "        # 2. SDL\n",
    "        sdl = DCCA_SDL(self.latent_dims,\n",
    "                       optimizer=self.optim,\n",
    "                       N=self.N, \n",
    "                       encoders = [self.encoder_1,self.encoder_2],\n",
    "                       lam=0.0001, \n",
    "                       lr=self.sdl_lr,\n",
    "                       dropout=0.05,\n",
    "                       objective=self.objective) \n",
    "\n",
    "        \n",
    "        \n",
    "        #define the trainer\n",
    "        \n",
    "        self.trainer = pl.Trainer(#default_root_dir=default_root_dir,\n",
    "                             logger = logger,\n",
    "                             max_epochs=self.epochs, #enable early stoppage instead of specifiying num epochs                           \n",
    "                             log_every_n_steps=1,\n",
    "                             val_check_interval = 1, #`Trainer(val_check_interval=1)` was configured so validation will run after every batch.\n",
    "                             \n",
    "                             callbacks=[\n",
    "                                checkpoint,\n",
    "                                 #pl.callbacks.early_stopping.EarlyStopping(monitor=\"val/l2\") # early stopage to reduce overfitting\n",
    "                             ],\n",
    "                            enable_progress_bar=enable_progress_bar,\n",
    "                            auto_lr_find = True\n",
    "                                )#,\n",
    "        \n",
    "        #callbacks=[pl.callbacks.early_stopping.EarlyStopping(monitor=\"train/sdl\")])# early stopage to reduce overfitting\n",
    "        \n",
    "        print('\\n','-'*20,'\\n Training SDL\\n','-'*20)\n",
    "        self.trainer.fit(sdl, self.train_loader,self.val_loader)\n",
    "        self.sdl = sdl\n",
    "        return None#sdl\n",
    "    \n",
    "    def train_dcca(self):\n",
    "        \n",
    "       \n",
    "        \n",
    "        # 2. DCCA\n",
    "        dcca = DCCA(self.latent_dims,\n",
    "                    optimizer=self.optim,\n",
    "                    encoders = [self.encoder_1,self.encoder_2],\n",
    "                    lr=self.dcca_lr,\n",
    "                    objective=self.objective) \n",
    "\n",
    "        #train\n",
    "        #tb_logger = pl_loggers.TensorBoardLogger(save_dir=\"pl_logs/dcca\")\n",
    "        trainer = pl.Trainer(default_root_dir=\"./dcca\",max_epochs=self.epochs,log_every_n_steps=1)#,\n",
    "        \n",
    "        #callbacks=[pl.callbacks.early_stopping.EarlyStopping(monitor=\"train/sdl\")])# early stopage to reduce overfitting\n",
    "\n",
    "        print('\\n','-'*20,'\\n Training DCCA\\n','-'*20)\n",
    "        trainer.fit(dcca, self.train_loader,self.val_loader)\n",
    "        \n",
    "        self.dcca = dcca\n",
    "        \n",
    "        return None #dcca\n",
    "    \n",
    "    \n",
    "    \n",
    "    def score(self,model,dataset): \n",
    "        \"\"\"\n",
    "        model: either 'cca', 'dcca', 'sdl'\n",
    "        dataset: 'train', 'test', or 'val'\n",
    "        \n",
    "        returns: correlation \n",
    "        \"\"\"\n",
    "       # for cca models \n",
    "        #m = eval(model)\n",
    "        \n",
    "        #specify data to transform\n",
    "        if dataset == 'train':\n",
    "            v1,v2, loader = self.v1_train, self.v2_train, self.train_loader\n",
    "        elif dataset == 'test':\n",
    "            v1,v2, loader = self.v1_test, self.v2_test, self.test_loader\n",
    "        elif dataset == 'val':\n",
    "            v1,v2, loader = self.v1_val, self.v2_val, self.val_loader\n",
    "        \n",
    "        if model == 'cca':\n",
    "            corr = self.cca.score([v1,v2])\n",
    "        \n",
    "        if model == 'sdl':\n",
    "            corr = self.sdl.score(loader)\n",
    "        \n",
    "        elif model == 'dcca':\n",
    "            corr = self.dcca.score(loader)\n",
    "       \n",
    "        return corr\n",
    "    def update_z_scores(self,dataset, z1,z2,cols):\n",
    "        #update train df with transformed z scores\n",
    "            if dataset == 'train':\n",
    "                \n",
    "                self.train_df[cols[0]] = [x for x in z1]\n",
    "                self.train_df[cols[1]] = [x for x in z2]\n",
    "                \n",
    "            #update test df\n",
    "            if dataset == 'test':\n",
    "                self.test_df[cols[0]] = [x for x in z1]\n",
    "                self.test_df[cols[1]] = [x for x in z2]\n",
    "            \n",
    "            #update val df\n",
    "            if dataset == 'val':\n",
    "                self.val_df[cols[0]] = [x for x in z1]\n",
    "                self.val_df[cols[1]] = [x for x in z2]\n",
    "            return None\n",
    "                \n",
    "        \n",
    "    \n",
    "    def transform(self,model,dataset):\n",
    "        \"\"\"\n",
    "        model: either 'cca', 'dcca', 'sdl': of course the model must have been fitted :)\n",
    "        loader: is similar data loader used to train either sdl/dcca\n",
    "        dataset: either 'train', 'test', 'val'\n",
    "        \n",
    "        returns transformed data; view1,view2\n",
    "        \"\"\"\n",
    "        \n",
    "        #specify data to transform\n",
    "        if dataset == 'train':\n",
    "            v1,v2, loader = self.v1_train, self.v2_train, self.train_loader\n",
    "        elif dataset == 'test':\n",
    "            v1,v2, loader = self.v1_test, self.v2_test, self.test_loader\n",
    "        elif dataset == 'val':\n",
    "            v1,v2, loader = self.v1_val, self.v2_val, self.val_loader\n",
    "        \n",
    "        \n",
    "        #specify the model for transformation\n",
    "        if model == 'cca':\n",
    "            z1,z2 = self.cca.transform([v1,v2]) #transform\n",
    "            self.update_z_scores(dataset,z1,z2,cols=['cca_z1','cca_z2']) # update the df with z scores            \n",
    "    \n",
    "            \n",
    "        if model == 'sdl':\n",
    "            z1,z2 = self.sdl.transform(loader)\n",
    "            self.update_z_scores(dataset,z1,z2,cols=['sdl_z1','sdl_z2'])\n",
    "                \n",
    "        \n",
    "        if model == 'dcca':\n",
    "            z1,z2 = self.dcca.transform(loader)\n",
    "            self.update_z_scores(dataset,z1,z2,cols=['dcca_z1','dcca_z2'])\n",
    "        \n",
    "        \n",
    "        return None##z1,z2; can be found in self.<df[model_z]>       \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a54ecaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 15\n"
     ]
    }
   ],
   "source": [
    "# Initiate deepcca objec\n",
    "Models = DeepCCA(df_all)\n",
    "\n",
    "# generate data loaders and cca v1,v2\n",
    "Models.deepcca_dataloaders()\n",
    "\n",
    "# set up the encoders\n",
    "Models.deepcca_encoders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "27f5d508",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#define the metric to monitor 'like the scorer for gridsearch'\n",
    "\n",
    "metrics = {\"loss\": \"val/l2\"}\n",
    "\n",
    "#creat a callback that will communicate with ray-tune\n",
    "ray_tune_callback = TuneReportCallback(metrics, on=\"validation_end\")\n",
    "\n",
    "# Defining a search space!\n",
    "config = {\n",
    "     \"optimizer\": tune.choice(['sgd', 'adam', 'adamw']),\n",
    " \n",
    "     \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "     \"batch_size\": tune.choice([128, 128*2, 128*3]),\n",
    "        \"latent_dims\": tune.choice([10,20,30]), \n",
    "    \"dropout\": tune.choice([0.05,0.1,0.15,0.2,0.25])\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c63ee115",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up logger\n",
    "version =  f'testing_ray_tune'\n",
    "   \n",
    "experiment_dir = './sdl_logs'\n",
    "    \n",
    "checkpoint = ModelCheckpoint(save_last=True,\n",
    "                                       monitor=\"val/l2\",\n",
    "                                       mode = 'min')\n",
    "    \n",
    "logger = TensorBoardLogger(save_dir=experiment_dir, \n",
    "                                   name='ray_tune',\n",
    "                                   version = version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2297f8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tune(config, epochs=10, gpus=0):\n",
    "    \n",
    "    \n",
    "    #set up logger\n",
    "    version =  f'testing_ray_tune'\n",
    "   \n",
    "    experiment_dir = './sdl_logs'\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(save_last=True,\n",
    "                                       monitor=\"val/l2\",\n",
    "                                       mode = 'min')\n",
    "    \n",
    "    logger = TensorBoardLogger(save_dir=experiment_dir, \n",
    "                                   name='ray_tune',\n",
    "                                   version = version)\n",
    "    \n",
    "    \n",
    "    sdl = DCCA_SDL(Models.latent_dims,\n",
    "                optimizer=config['optimizer'],\n",
    "                N=Models.N, \n",
    "                encoders = [Models.encoder_1,Models.encoder_2],\n",
    "                lam=0.0001, \n",
    "                lr=Models.sdl_lr,\n",
    "                dropout=0.05,\n",
    "                objective=Models.objective)\n",
    "    \n",
    "    trainer = pl.Trainer(#default_root_dir=default_root_dir,\n",
    "    logger = logger,\n",
    "    max_epochs=10, #enable early stoppage instead of specifiying num epochs                           \n",
    "    log_every_n_steps=1,\n",
    "    val_check_interval = 1, #`Trainer(val_check_interval=1)` was configured so validation will run after every batch.\n",
    "    callbacks=[\n",
    "        checkpoint,\n",
    "        pl.callbacks.early_stopping.EarlyStopping(monitor=\"val/l2\"), # early stopage to reduce overfitting\n",
    "        ray_tune_callback\n",
    "              ],\n",
    "    enable_progress_bar=True,\n",
    "    auto_lr_find = True\n",
    "    )\n",
    "    \n",
    "    trainer.fit(sdl,Models.train_loader,Models.val_loader)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd5cd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_tune pid=2676)\u001b[0m \r",
      "Epoch 0:  50%|█████     | 1/2 [01:03<01:03, 63.11s/it]\r",
      "Epoch 0:  50%|█████     | 1/2 [01:03<01:03, 63.12s/it]\n",
      "\u001b[2m\u001b[36m(train_tune pid=2676)\u001b[0m \r",
      "Epoch 0:  50%|█████     | 1/2 [01:03<01:03, 63.12s/it, loss=2.02, v_num=tune, train/objective=2.020, train/l2=2.010, train/sdl=23.70]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_tune pid=2676)\u001b[0m 2023-01-09 18:24:57.991774: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "\u001b[2m\u001b[36m(train_tune pid=2676)\u001b[0m 2023-01-09 18:24:57.991878: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_tune pid=2676)\u001b[0m \n",
      "Validation: 0it [00:00, ?it/s]\u001b[Am \n",
      "\u001b[2m\u001b[36m(train_tune pid=2676)\u001b[0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 2/2 [01:09<00:00, 34.86s/it, loss=2.02, v_num=tune, train/objective=2.020, train/l2=2.010, train/sdl=23.70]\n",
      "Epoch 0:  50%|█████     | 1/2 [01:01<01:01, 61.95s/it, loss=2.02, v_num=tune, train/objective=2.020, train/l2=2.010, train/sdl=23.70]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_tune pid=10072)\u001b[0m 2023-01-09 18:25:09.094269: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "\u001b[2m\u001b[36m(train_tune pid=10072)\u001b[0m 2023-01-09 18:25:09.094364: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_tune pid=10072)\u001b[0m \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "\u001b[2m\u001b[36m(train_tune pid=10072)\u001b[0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 2/2 [01:08<00:00, 34.02s/it, loss=2.02, v_num=tune, train/objective=2.020, train/l2=2.010, train/sdl=23.70]\n",
      "Epoch 0: 100%|██████████| 2/2 [01:08<00:00, 34.02s/it, loss=2.02, v_num=tune, train/objective=2.020, train/l2=2.010, train/sdl=23.70]\n",
      "Epoch 0:  50%|█████     | 1/2 [00:58<00:58, 58.83s/it, loss=2.02, v_num=tune, train/objective=2.020, train/l2=2.010, train/sdl=23.70]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_tune pid=2404)\u001b[0m 2023-01-09 18:25:18.440240: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "\u001b[2m\u001b[36m(train_tune pid=2404)\u001b[0m 2023-01-09 18:25:18.440324: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_tune pid=2404)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_tune pid=2404)\u001b[0m \r",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(train_tune pid=2404)\u001b[0m \r",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(train_tune pid=2404)\u001b[0m \r",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(train_tune pid=2404)\u001b[0m \r",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 115.93it/s]\u001b[A\r",
      "Epoch 0: 100%|██████████| 2/2 [01:04<00:00, 32.35s/it, loss=2.02, v_num=tune, train/objective=2.020, train/l2=2.010, train/sdl=23.70]\r",
      "Epoch 0: 100%|██████████| 2/2 [01:04<00:00, 32.35s/it, loss=2.02, v_num=tune, train/objective=2.020, train/l2=2.010, train/sdl=23.70]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_tune pid=14980)\u001b[0m 2023-01-09 18:25:25.829970: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "\u001b[2m\u001b[36m(train_tune pid=14980)\u001b[0m 2023-01-09 18:25:25.830030: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  50%|█████     | 1/2 [00:51<00:51, 51.94s/it, loss=2.02, v_num=tune, train/objective=2.020, train/l2=2.010, train/sdl=23.70]\n",
      "\u001b[2m\u001b[36m(train_tune pid=14980)\u001b[0m \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(train_tune pid=14980)\u001b[0m \n",
      "Epoch 0: 100%|██████████| 2/2 [00:56<00:00, 28.30s/it, loss=2.02, v_num=tune, train/objective=2.020, train/l2=2.010, train/sdl=23.70]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=~/ray_results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0ec4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-01-09 18:29:31</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:53.41        </td></tr>\n",
       "<tr><td>Memory:      </td><td>10.0/15.4 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/4.28 GiB heap, 0.0/2.14 GiB objects\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout</th><th style=\"text-align: right;\">  latent_dims</th><th style=\"text-align: right;\">         lr</th><th>optimizer  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_tune_0d44d_00000</td><td>RUNNING </td><td>127.0.0.1:19240</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">     0.25</td><td style=\"text-align: right;\">           20</td><td style=\"text-align: right;\">0.000138871</td><td>sgd        </td></tr>\n",
       "<tr><td>train_tune_0d44d_00001</td><td>RUNNING </td><td>127.0.0.1:9284 </td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">     0.1 </td><td style=\"text-align: right;\">           30</td><td style=\"text-align: right;\">0.088153   </td><td>adamw      </td></tr>\n",
       "<tr><td>train_tune_0d44d_00002</td><td>RUNNING </td><td>127.0.0.1:6684 </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">     0.1 </td><td style=\"text-align: right;\">           30</td><td style=\"text-align: right;\">0.000896127</td><td>adamw      </td></tr>\n",
       "<tr><td>train_tune_0d44d_00003</td><td>RUNNING </td><td>127.0.0.1:12908</td><td style=\"text-align: right;\">         384</td><td style=\"text-align: right;\">     0.2 </td><td style=\"text-align: right;\">           10</td><td style=\"text-align: right;\">0.000108108</td><td>adamw      </td></tr>\n",
       "<tr><td>train_tune_0d44d_00004</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">     0.2 </td><td style=\"text-align: right;\">           20</td><td style=\"text-align: right;\">0.00027433 </td><td>adamw      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-09 18:28:39,839\tWARNING util.py:244 -- The `start_trial` operation took 0.618 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m `Trainer(val_check_interval=1)` was configured so validation will run after every batch.\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:354: LightningDeprecationWarning: The `on_configure_sharded_model` callback hook was deprecated in v1.6 and will be removed in v1.8. Use `setup()` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:359: LightningDeprecationWarning: The `on_before_accelerator_backend_setup` callback hook was deprecated in v1.6 and will be removed in v1.8. Use `setup()` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:364: LightningDeprecationWarning: `TuneReportCallback.on_load_checkpoint` will change its signature and behavior in v1.8. If you wish to load the state of the callback, use `load_state_dict` instead. In v1.8 `on_load_checkpoint(..., checkpoint)` will receive the entire loaded checkpoint dictionary instead of callback state.\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_start` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_end` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m   | Name     | Type       | Params\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m ----------------------------------------\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m 0 | encoders | ModuleList | 852 K \n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m 1 | mse      | MSELoss    | 0     \n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m 2 | bns      | ModuleList | 0     \n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m ----------------------------------------\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m 852 K     Trainable params\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m 852 K     Total params\n",
      "\u001b[2m\u001b[36m(train_tune pid=19240)\u001b[0m 3.409     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s] \n",
      "Sanity Checking DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 63.70it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m `Trainer(val_check_interval=1)` was configured so validation will run after every batch.\n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:354: LightningDeprecationWarning: The `on_configure_sharded_model` callback hook was deprecated in v1.6 and will be removed in v1.8. Use `setup()` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:359: LightningDeprecationWarning: The `on_before_accelerator_backend_setup` callback hook was deprecated in v1.6 and will be removed in v1.8. Use `setup()` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:364: LightningDeprecationWarning: `TuneReportCallback.on_load_checkpoint` will change its signature and behavior in v1.8. If you wish to load the state of the callback, use `load_state_dict` instead. In v1.8 `on_load_checkpoint(..., checkpoint)` will receive the entire loaded checkpoint dictionary instead of callback state.\n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_start` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_end` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m   | Name     | Type       | Params\n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m ----------------------------------------\n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m 0 | encoders | ModuleList | 852 K \n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m 1 | mse      | MSELoss    | 0     \n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m 2 | bns      | ModuleList | 0     \n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m ----------------------------------------\n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m 852 K     Trainable params\n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m 852 K     Total params\n",
      "\u001b[2m\u001b[36m(train_tune pid=9284)\u001b[0m 3.409     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]\n",
      "Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s]                             \n",
      "Sanity Checking DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 65.14it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m `Trainer(val_check_interval=1)` was configured so validation will run after every batch.\n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:354: LightningDeprecationWarning: The `on_configure_sharded_model` callback hook was deprecated in v1.6 and will be removed in v1.8. Use `setup()` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:359: LightningDeprecationWarning: The `on_before_accelerator_backend_setup` callback hook was deprecated in v1.6 and will be removed in v1.8. Use `setup()` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:364: LightningDeprecationWarning: `TuneReportCallback.on_load_checkpoint` will change its signature and behavior in v1.8. If you wish to load the state of the callback, use `load_state_dict` instead. In v1.8 `on_load_checkpoint(..., checkpoint)` will receive the entire loaded checkpoint dictionary instead of callback state.\n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_start` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_end` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m   | Name     | Type       | Params\n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m ----------------------------------------\n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m 0 | encoders | ModuleList | 852 K \n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m 1 | mse      | MSELoss    | 0     \n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m 2 | bns      | ModuleList | 0     \n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m ----------------------------------------\n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m 852 K     Trainable params\n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m 852 K     Total params\n",
      "\u001b[2m\u001b[36m(train_tune pid=6684)\u001b[0m 3.409     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]\n",
      "Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s]                             \n",
      "Sanity Checking DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 61.95it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m `Trainer(val_check_interval=1)` was configured so validation will run after every batch.\n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:354: LightningDeprecationWarning: The `on_configure_sharded_model` callback hook was deprecated in v1.6 and will be removed in v1.8. Use `setup()` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:359: LightningDeprecationWarning: The `on_before_accelerator_backend_setup` callback hook was deprecated in v1.6 and will be removed in v1.8. Use `setup()` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:364: LightningDeprecationWarning: `TuneReportCallback.on_load_checkpoint` will change its signature and behavior in v1.8. If you wish to load the state of the callback, use `load_state_dict` instead. In v1.8 `on_load_checkpoint(..., checkpoint)` will receive the entire loaded checkpoint dictionary instead of callback state.\n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_start` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_end` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m   | Name     | Type       | Params\n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m ----------------------------------------\n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m 0 | encoders | ModuleList | 852 K \n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m 1 | mse      | MSELoss    | 0     \n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m 2 | bns      | ModuleList | 0     \n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m ----------------------------------------\n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m 852 K     Trainable params\n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m 852 K     Total params\n",
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m 3.409     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_tune pid=12908)\u001b[0m \r",
      "Sanity Checking: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "tuner = tune.Tuner(\n",
    "    tune.with_resources(train_tune, {\"cpu\": 1, \"extra_cpu\": 4}),\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric='loss',\n",
    "        mode=\"min\",\n",
    "        num_samples=5,\n",
    "    ),\n",
    "    param_space=config\n",
    ")\n",
    "\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5166b952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'optimizer': 'sgd',\n",
       " 'lr': 0.0004018831832317727,\n",
       " 'batch_size': 256,\n",
       " 'latent_dims': 10,\n",
       " 'dropout': 0.25}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.get_best_result().config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8b16ebc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-01-09 16:31:13</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:32.72        </td></tr>\n",
       "<tr><td>Memory:      </td><td>14.7/15.4 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 5.0/8 CPUs, 0/0 GPUs, 0.0/4.28 GiB heap, 0.0/2.14 GiB objects\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  : ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
       "  \n",
       "  \n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout</th><th style=\"text-align: right;\">  latent_dims</th><th style=\"text-align: right;\">         lr</th><th>optimizer  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_tune_6f2c2_00000</td><td>RUNNING </td><td>127.0.0.1:9548 </td><td style=\"text-align: right;\">         384</td><td style=\"text-align: right;\">     0.1 </td><td style=\"text-align: right;\">           30</td><td style=\"text-align: right;\">0.000118526</td><td>adam       </td></tr>\n",
       "<tr><td>train_tune_6f2c2_00001</td><td>RUNNING </td><td>127.0.0.1:12892</td><td style=\"text-align: right;\">         384</td><td style=\"text-align: right;\">     0.2 </td><td style=\"text-align: right;\">           30</td><td style=\"text-align: right;\">0.000442803</td><td>sgd        </td></tr>\n",
       "<tr><td>train_tune_6f2c2_00002</td><td>RUNNING </td><td>127.0.0.1:6540 </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">     0.2 </td><td style=\"text-align: right;\">           10</td><td style=\"text-align: right;\">0.000483768</td><td>adam       </td></tr>\n",
       "<tr><td>train_tune_6f2c2_00003</td><td>RUNNING </td><td>127.0.0.1:10008</td><td style=\"text-align: right;\">         384</td><td style=\"text-align: right;\">     0.25</td><td style=\"text-align: right;\">           10</td><td style=\"text-align: right;\">0.000340639</td><td>adam       </td></tr>\n",
       "<tr><td>train_tune_6f2c2_00004</td><td>RUNNING </td><td>127.0.0.1:13368</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">     0.15</td><td style=\"text-align: right;\">           30</td><td style=\"text-align: right;\">0.00451404 </td><td>adam       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m `Trainer(val_check_interval=1)` was configured so validation will run after every batch.\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:354: LightningDeprecationWarning: The `on_configure_sharded_model` callback hook was deprecated in v1.6 and will be removed in v1.8. Use `setup()` instead.\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:359: LightningDeprecationWarning: The `on_before_accelerator_backend_setup` callback hook was deprecated in v1.6 and will be removed in v1.8. Use `setup()` instead.\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:364: LightningDeprecationWarning: `TuneReportCallback.on_load_checkpoint` will change its signature and behavior in v1.8. If you wish to load the state of the callback, use `load_state_dict` instead. In v1.8 `on_load_checkpoint(..., checkpoint)` will receive the entire loaded checkpoint dictionary instead of callback state.\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_start` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_end` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m \n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m   | Name     | Type       | Params\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m ----------------------------------------\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m 0 | encoders | ModuleList | 852 K \n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m 1 | mse      | MSELoss    | 0     \n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m 2 | bns      | ModuleList | 0     \n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m ----------------------------------------\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m 852 K     Trainable params\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m 852 K     Total params\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m 3.409     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]\n",
      "Sanity Checking DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 102.79it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m `Trainer(val_check_interval=1)` was configured so validation will run after every batch.\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:354: LightningDeprecationWarning: The `on_configure_sharded_model` callback hook was deprecated in v1.6 and will be removed in v1.8. Use `setup()` instead.\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:359: LightningDeprecationWarning: The `on_before_accelerator_backend_setup` callback hook was deprecated in v1.6 and will be removed in v1.8. Use `setup()` instead.\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:364: LightningDeprecationWarning: `TuneReportCallback.on_load_checkpoint` will change its signature and behavior in v1.8. If you wish to load the state of the callback, use `load_state_dict` instead. In v1.8 `on_load_checkpoint(..., checkpoint)` will receive the entire loaded checkpoint dictionary instead of callback state.\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_start` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_end` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m \n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m   | Name     | Type       | Params\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m ----------------------------------------\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m 0 | encoders | ModuleList | 852 K \n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m 1 | mse      | MSELoss    | 0     \n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m 2 | bns      | ModuleList | 0     \n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m ----------------------------------------\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m 852 K     Trainable params\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m 852 K     Total params\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m 3.409     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]\n",
      "Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s]                              \n",
      "Sanity Checking DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 65.68it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m `Trainer(val_check_interval=1)` was configured so validation will run after every batch.\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:354: LightningDeprecationWarning: The `on_configure_sharded_model` callback hook was deprecated in v1.6 and will be removed in v1.8. Use `setup()` instead.\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:359: LightningDeprecationWarning: The `on_before_accelerator_backend_setup` callback hook was deprecated in v1.6 and will be removed in v1.8. Use `setup()` instead.\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:364: LightningDeprecationWarning: `TuneReportCallback.on_load_checkpoint` will change its signature and behavior in v1.8. If you wish to load the state of the callback, use `load_state_dict` instead. In v1.8 `on_load_checkpoint(..., checkpoint)` will receive the entire loaded checkpoint dictionary instead of callback state.\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_start` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_end` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m \n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m   | Name     | Type       | Params\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m ----------------------------------------\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m 0 | encoders | ModuleList | 852 K \n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m 1 | mse      | MSELoss    | 0     \n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m 2 | bns      | ModuleList | 0     \n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m ----------------------------------------\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m 852 K     Trainable params\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m 852 K     Total params\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m 3.409     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]\n",
      "Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s]                             \n",
      "Sanity Checking DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 51.96it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m `Trainer(val_check_interval=1)` was configured so validation will run after every batch.\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:354: LightningDeprecationWarning: The `on_configure_sharded_model` callback hook was deprecated in v1.6 and will be removed in v1.8. Use `setup()` instead.\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:359: LightningDeprecationWarning: The `on_before_accelerator_backend_setup` callback hook was deprecated in v1.6 and will be removed in v1.8. Use `setup()` instead.\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:364: LightningDeprecationWarning: `TuneReportCallback.on_load_checkpoint` will change its signature and behavior in v1.8. If you wish to load the state of the callback, use `load_state_dict` instead. In v1.8 `on_load_checkpoint(..., checkpoint)` will receive the entire loaded checkpoint dictionary instead of callback state.\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_start` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_end` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m \n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m   | Name     | Type       | Params\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m ----------------------------------------\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m 0 | encoders | ModuleList | 852 K \n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m 1 | mse      | MSELoss    | 0     \n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m 2 | bns      | ModuleList | 0     \n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m ----------------------------------------\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m 852 K     Trainable params\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m 852 K     Total params\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m 3.409     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]\n",
      "Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s]                             \n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 52.17it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m `Trainer(val_check_interval=1)` was configured so validation will run after every batch.\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:354: LightningDeprecationWarning: The `on_configure_sharded_model` callback hook was deprecated in v1.6 and will be removed in v1.8. Use `setup()` instead.\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:359: LightningDeprecationWarning: The `on_before_accelerator_backend_setup` callback hook was deprecated in v1.6 and will be removed in v1.8. Use `setup()` instead.\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:364: LightningDeprecationWarning: `TuneReportCallback.on_load_checkpoint` will change its signature and behavior in v1.8. If you wish to load the state of the callback, use `load_state_dict` instead. In v1.8 `on_load_checkpoint(..., checkpoint)` will receive the entire loaded checkpoint dictionary instead of callback state.\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_start` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_end` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m \n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m   | Name     | Type       | Params\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m ----------------------------------------\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m 0 | encoders | ModuleList | 852 K \n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m 1 | mse      | MSELoss    | 0     \n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m 2 | bns      | ModuleList | 0     \n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m ----------------------------------------\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m 852 K     Trainable params\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m 852 K     Total params\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m 3.409     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]\n",
      "Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s]                             \n",
      "Sanity Checking DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 81.74it/s]\n",
      "Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s]                             \n",
      "Epoch 0:  50%|█████     | 1/2 [00:55<00:55, 55.03s/it, loss=2.02, v_num=tune, train/objective=2.020, train/l2=2.010, train/sdl=23.70]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m 2023-01-09 16:31:10.394260: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m 2023-01-09 16:31:10.395681: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m \n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m \r",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m \n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m \r",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m \r",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(func pid=9548)\u001b[0m \r",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 180.44it/s]\u001b[A\r",
      "Epoch 0: 100%|██████████| 2/2 [01:03<00:00, 31.51s/it, loss=2.02, v_num=tune, train/objective=2.020, train/l2=2.010, train/sdl=23.70]\r",
      "Epoch 0: 100%|██████████| 2/2 [01:03<00:00, 31.51s/it, loss=2.02, v_num=tune, train/objective=2.020, train/l2=2.010, train/sdl=23.70]\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ray\\tune\\execution\\trial_runner.py\", line 948, in _wait_and_handle_event\n    self._on_training_result(\n  File \"C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ray\\tune\\execution\\trial_runner.py\", line 1073, in _on_training_result\n    self._process_trial_results(trial, result)\n  File \"C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ray\\tune\\execution\\trial_runner.py\", line 1156, in _process_trial_results\n    decision = self._process_trial_result(trial, result)\n  File \"C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ray\\tune\\execution\\trial_runner.py\", line 1193, in _process_trial_result\n    self._validate_result_metrics(flat_result)\n  File \"C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ray\\tune\\execution\\trial_runner.py\", line 1289, in _validate_result_metrics\n    raise ValueError(\nValueError: Trial returned a result which did not include the specified metric(s) `val/l2` that `tune.TuneConfig()` expects. Make sure your calls to `tune.report()` include the metric, or set the TUNE_DISABLE_STRICT_METRIC_CHECKING environment variable to 1. Result: {'loss': 2.2113847732543945, 'time_this_iter_s': 77.1507637500763, 'done': False, 'timesteps_total': None, 'episodes_total': None, 'training_iteration': 1, 'trial_id': '6f2c2_00000', 'experiment_id': 'e93afc2c90f940f6879408e3ce4f0b94', 'date': '2023-01-09_16-31-18', 'timestamp': 1673278278, 'time_total_s': 77.1507637500763, 'pid': 9548, 'hostname': 'LLL', 'node_ip': '127.0.0.1', 'time_since_restore': 77.1507637500763, 'timesteps_since_restore': 0, 'iterations_since_restore': 1, 'warmup_time': 0.009219884872436523, 'config/optimizer': 'adam', 'config/lr': 0.0001185261695250476, 'config/batch_size': 384, 'config/latent_dims': 30, 'config/dropout': 0.1}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ray\\tune\\execution\\trial_runner.py\u001b[0m in \u001b[0;36m_wait_and_handle_event\u001b[1;34m(self, next_trial)\u001b[0m\n\u001b[0;32m    947\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_ExecutorEventType\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAINING_RESULT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 948\u001b[1;33m                         self._on_training_result(\n\u001b[0m\u001b[0;32m    949\u001b[0m                             \u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_ExecutorEvent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKEY_FUTURE_RESULT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ray\\tune\\execution\\trial_runner.py\u001b[0m in \u001b[0;36m_on_training_result\u001b[1;34m(self, trial, result)\u001b[0m\n\u001b[0;32m   1072\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mwarn_if_slow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"process_trial_result\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1073\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_trial_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ray\\tune\\execution\\trial_runner.py\u001b[0m in \u001b[0;36m_process_trial_results\u001b[1;34m(self, trial, results)\u001b[0m\n\u001b[0;32m   1155\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mwarn_if_slow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"process_trial_result\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1156\u001b[1;33m                     \u001b[0mdecision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_trial_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1157\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mdecision\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ray\\tune\\execution\\trial_runner.py\u001b[0m in \u001b[0;36m_process_trial_result\u001b[1;34m(self, trial, result)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mflat_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflatten_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1193\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_result_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ray\\tune\\execution\\trial_runner.py\u001b[0m in \u001b[0;36m_validate_result_metrics\u001b[1;34m(self, result)\u001b[0m\n\u001b[0;32m   1288\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreport_metric\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1289\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m   1290\u001b[0m                     \u001b[1;34m\"Trial returned a result which did not include the \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Trial returned a result which did not include the specified metric(s) `val/l2` that `tune.TuneConfig()` expects. Make sure your calls to `tune.report()` include the metric, or set the TUNE_DISABLE_STRICT_METRIC_CHECKING environment variable to 1. Result: {'loss': 2.2113847732543945, 'time_this_iter_s': 77.1507637500763, 'done': False, 'timesteps_total': None, 'episodes_total': None, 'training_iteration': 1, 'trial_id': '6f2c2_00000', 'experiment_id': 'e93afc2c90f940f6879408e3ce4f0b94', 'date': '2023-01-09_16-31-18', 'timestamp': 1673278278, 'time_total_s': 77.1507637500763, 'pid': 9548, 'hostname': 'LLL', 'node_ip': '127.0.0.1', 'time_since_restore': 77.1507637500763, 'timesteps_since_restore': 0, 'iterations_since_restore': 1, 'warmup_time': 0.009219884872436523, 'config/optimizer': 'adam', 'config/lr': 0.0001185261695250476, 'config/batch_size': 384, 'config/latent_dims': 30, 'config/dropout': 0.1}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5000/2730543805.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfunctools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m sdl_ray = tune.run(\n\u001b[0m\u001b[0;32m      3\u001b[0m  \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_tune\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgpus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"val/l2\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'min'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m  )\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ray\\tune\\tune.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, chdir_to_trial_dir, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, max_concurrent_trials, _experiment_checkpoint_dir, _remote, _remote_string_queue)\u001b[0m\n\u001b[0;32m    724\u001b[0m     )\n\u001b[0;32m    725\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"signal\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 726\u001b[1;33m         \u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    727\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhas_verbosity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVerbosity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV1_EXPERIMENT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m             \u001b[0m_report_progress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrunner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress_reporter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ray\\tune\\execution\\trial_runner.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    979\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Got new trial to run: {next_trial}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 981\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wait_and_handle_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_trial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    982\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    983\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stop_experiment_if_needed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ray\\tune\\execution\\trial_runner.py\u001b[0m in \u001b[0;36m_wait_and_handle_event\u001b[1;34m(self, next_trial)\u001b[0m\n\u001b[0;32m    958\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 960\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_exc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTuneError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ray\\tune\\execution\\trial_runner.py\", line 948, in _wait_and_handle_event\n    self._on_training_result(\n  File \"C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ray\\tune\\execution\\trial_runner.py\", line 1073, in _on_training_result\n    self._process_trial_results(trial, result)\n  File \"C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ray\\tune\\execution\\trial_runner.py\", line 1156, in _process_trial_results\n    decision = self._process_trial_result(trial, result)\n  File \"C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ray\\tune\\execution\\trial_runner.py\", line 1193, in _process_trial_result\n    self._validate_result_metrics(flat_result)\n  File \"C:\\Users\\lmeli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ray\\tune\\execution\\trial_runner.py\", line 1289, in _validate_result_metrics\n    raise ValueError(\nValueError: Trial returned a result which did not include the specified metric(s) `val/l2` that `tune.TuneConfig()` expects. Make sure your calls to `tune.report()` include the metric, or set the TUNE_DISABLE_STRICT_METRIC_CHECKING environment variable to 1. Result: {'loss': 2.2113847732543945, 'time_this_iter_s': 77.1507637500763, 'done': False, 'timesteps_total': None, 'episodes_total': None, 'training_iteration': 1, 'trial_id': '6f2c2_00000', 'experiment_id': 'e93afc2c90f940f6879408e3ce4f0b94', 'date': '2023-01-09_16-31-18', 'timestamp': 1673278278, 'time_total_s': 77.1507637500763, 'pid': 9548, 'hostname': 'LLL', 'node_ip': '127.0.0.1', 'time_since_restore': 77.1507637500763, 'timesteps_since_restore': 0, 'iterations_since_restore': 1, 'warmup_time': 0.009219884872436523, 'config/optimizer': 'adam', 'config/lr': 0.0001185261695250476, 'config/batch_size': 384, 'config/latent_dims': 30, 'config/dropout': 0.1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m \r",
      "Epoch 0:  50%|█████     | 1/2 [00:55<00:55, 55.73s/it]\r",
      "Epoch 0:  50%|█████     | 1/2 [00:55<00:55, 55.73s/it]\r",
      "Epoch 0:  50%|█████     | 1/2 [00:55<00:55, 55.73s/it, loss=2.02, v_num=tune, train/objective=2.020, train/l2=2.010, train/sdl=23.70]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m 2023-01-09 16:31:21.014467: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m 2023-01-09 16:31:21.014541: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(func pid=12892)\u001b[0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 2/2 [01:01<00:00, 30.76s/it, loss=2.02, v_num=tune, train/objective=2.020, train/l2=2.010, train/sdl=23.70]\n",
      "Epoch 0:  50%|█████     | 1/2 [00:53<00:53, 53.55s/it, loss=2.02, v_num=tune, train/objective=2.020, train/l2=2.010, train/sdl=23.70]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m 2023-01-09 16:31:29.608396: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m 2023-01-09 16:31:29.608467: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m \n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m \r",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m \r",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m \r",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(func pid=6540)\u001b[0m \r",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 105.88it/s]\u001b[A\r",
      "Epoch 0: 100%|██████████| 2/2 [00:59<00:00, 29.56s/it, loss=2.02, v_num=tune, train/objective=2.020, train/l2=2.010, train/sdl=23.70]\r",
      "Epoch 0: 100%|██████████| 2/2 [00:59<00:00, 29.56s/it, loss=2.02, v_num=tune, train/objective=2.020, train/l2=2.010, train/sdl=23.70]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m 2023-01-09 16:31:39.854701: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m 2023-01-09 16:31:39.854760: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  50%|█████     | 1/2 [00:50<00:50, 50.75s/it, loss=2.02, v_num=tune, train/objective=2.020, train/l2=2.010, train/sdl=23.70]\n",
      "\u001b[2m\u001b[36m(func pid=10008)\u001b[0m \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 2/2 [00:55<00:00, 27.63s/it, loss=2.02, v_num=tune, train/objective=2.020, train/l2=2.010, train/sdl=23.70]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m 2023-01-09 16:31:46.753250: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m 2023-01-09 16:31:46.753326: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  50%|█████     | 1/2 [00:46<00:46, 46.49s/it, loss=2.02, v_num=tune, train/objective=2.020, train/l2=2.010, train/sdl=23.70]\n",
      "\u001b[2m\u001b[36m(func pid=13368)\u001b[0m \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 2/2 [00:52<00:00, 26.05s/it, loss=2.02, v_num=tune, train/objective=2.020, train/l2=2.010, train/sdl=23.70]\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "sdl_ray = tune.run(\n",
    " partial(train_tune,gpus=4),config=config, num_samples=5, metric=\"val/l2\", mode='min'\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "679242ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'optimizer': 'sgd',\n",
       " 'lr': 0.00038572205301596,\n",
       " 'batch_size': 128,\n",
       " 'latent_dims': 10,\n",
       " 'dropout': 0.25}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdl_ray.get_best_config(\"loss\", 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b9985f3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "To fetch the `best_config`, pass a `metric` and `mode` parameter to `tune.run()`. Alternatively, use the `get_best_config(metric, mode)` method to set the metric and mode explicitly.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5000/124878913.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msdl_ray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ray\\tune\\analysis\\experiment_analysis.py\u001b[0m in \u001b[0;36mbest_config\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \"\"\"\n\u001b[0;32m    224\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_metric\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_mode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    226\u001b[0m                 \u001b[1;34m\"To fetch the `best_config`, pass a `metric` and `mode` \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m                 \u001b[1;34m\"parameter to `tune.run()`. Alternatively, use the \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: To fetch the `best_config`, pass a `metric` and `mode` parameter to `tune.run()`. Alternatively, use the `get_best_config(metric, mode)` method to set the metric and mode explicitly."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=9248)\u001b[0m 2023-01-09 16:14:15.366168: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "\u001b[2m\u001b[36m(func pid=9248)\u001b[0m 2023-01-09 16:14:15.366243: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  50%|█████     | 1/2 [00:48<00:48, 48.72s/it, loss=2.02, v_num=tune, train/objective=2.020, train/l2=2.010, train/sdl=23.70]\n",
      "\u001b[2m\u001b[36m(func pid=9248)\u001b[0m \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 2/2 [00:53<00:00, 26.92s/it, loss=2.02, v_num=tune, train/objective=2.020, train/l2=2.010, train/sdl=23.70]\n"
     ]
    }
   ],
   "source": [
    "sdl_ray.best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b949e9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### END of ray-tune-test ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4af14604",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Files:\n",
    "    def __init__(self, filename):\n",
    "        self.file = filename\n",
    "        \n",
    "    def write_to_file(self, data):\n",
    "        with open(self.file, 'wb') as f:\n",
    "            pickle.dump(data, f) \n",
    "        return None\n",
    "    \n",
    "    def load_pickle(self):\n",
    "        data = pd.read_pickle(self.file)\n",
    "        return data\n",
    "    \n",
    "    def load_csv(self, sep, usecols=None):\n",
    "        data = pd.read_csv(self.file, sep=sep, usecols=usecols)\n",
    "        return data\n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12750a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tanis = Files('./GNPS_15_12_2021_pos_tanimoto_scores.pickle').load_pickle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45d31c26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>inchikey14</th>\n",
       "      <th>LFTLOKWAGJYHHR</th>\n",
       "      <th>BQDXDGDOYPUUOD</th>\n",
       "      <th>VEPUCZUJLKAVNM</th>\n",
       "      <th>PXPSEALQIQRPQY</th>\n",
       "      <th>HDZVRBPBPCZCJG</th>\n",
       "      <th>SXJIZQPZESTWLD</th>\n",
       "      <th>JDOFZOKGCYYUER</th>\n",
       "      <th>WGTCMJBJRPKENJ</th>\n",
       "      <th>FCCDDURTIIUXBY</th>\n",
       "      <th>FDLLEBFMOIHMNM</th>\n",
       "      <th>...</th>\n",
       "      <th>RJAHLSXSRQXGGI</th>\n",
       "      <th>VKJTXCWIQDBMLE</th>\n",
       "      <th>NFIHKFSODJJLGC</th>\n",
       "      <th>NHLBOKNHQIEJIH</th>\n",
       "      <th>QABASLXUKXNHMC</th>\n",
       "      <th>XGVJWXAYKUHDOO</th>\n",
       "      <th>MNKNQKOOKLVXDB</th>\n",
       "      <th>CQKNELOTFUSOTP</th>\n",
       "      <th>MHCYVCDXRQGUFW</th>\n",
       "      <th>NMCMVEXMLSARCJ</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inchikey14</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LFTLOKWAGJYHHR</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.057353</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.053269</td>\n",
       "      <td>0.069264</td>\n",
       "      <td>0.055453</td>\n",
       "      <td>0.048193</td>\n",
       "      <td>0.053296</td>\n",
       "      <td>0.052863</td>\n",
       "      <td>0.056204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049612</td>\n",
       "      <td>0.054762</td>\n",
       "      <td>0.053929</td>\n",
       "      <td>0.060065</td>\n",
       "      <td>0.049683</td>\n",
       "      <td>0.052980</td>\n",
       "      <td>0.049046</td>\n",
       "      <td>0.095833</td>\n",
       "      <td>0.050964</td>\n",
       "      <td>0.050159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BQDXDGDOYPUUOD</th>\n",
       "      <td>0.057353</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.162866</td>\n",
       "      <td>0.215026</td>\n",
       "      <td>0.242169</td>\n",
       "      <td>0.176221</td>\n",
       "      <td>0.296270</td>\n",
       "      <td>0.195915</td>\n",
       "      <td>0.089888</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185547</td>\n",
       "      <td>0.430151</td>\n",
       "      <td>0.180851</td>\n",
       "      <td>0.218014</td>\n",
       "      <td>0.321244</td>\n",
       "      <td>0.297297</td>\n",
       "      <td>0.272672</td>\n",
       "      <td>0.147776</td>\n",
       "      <td>0.317369</td>\n",
       "      <td>0.253207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VEPUCZUJLKAVNM</th>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.162866</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.286316</td>\n",
       "      <td>0.113158</td>\n",
       "      <td>0.152310</td>\n",
       "      <td>0.208607</td>\n",
       "      <td>0.264908</td>\n",
       "      <td>0.082418</td>\n",
       "      <td>0.186620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.157480</td>\n",
       "      <td>0.167925</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.152523</td>\n",
       "      <td>0.228311</td>\n",
       "      <td>0.251228</td>\n",
       "      <td>0.226978</td>\n",
       "      <td>0.080844</td>\n",
       "      <td>0.289835</td>\n",
       "      <td>0.231393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PXPSEALQIQRPQY</th>\n",
       "      <td>0.053269</td>\n",
       "      <td>0.215026</td>\n",
       "      <td>0.286316</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.134357</td>\n",
       "      <td>0.185499</td>\n",
       "      <td>0.342992</td>\n",
       "      <td>0.344860</td>\n",
       "      <td>0.081680</td>\n",
       "      <td>0.257655</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243169</td>\n",
       "      <td>0.248216</td>\n",
       "      <td>0.351724</td>\n",
       "      <td>0.207021</td>\n",
       "      <td>0.299073</td>\n",
       "      <td>0.376868</td>\n",
       "      <td>0.351122</td>\n",
       "      <td>0.073363</td>\n",
       "      <td>0.362462</td>\n",
       "      <td>0.374658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HDZVRBPBPCZCJG</th>\n",
       "      <td>0.069264</td>\n",
       "      <td>0.242169</td>\n",
       "      <td>0.113158</td>\n",
       "      <td>0.134357</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.145266</td>\n",
       "      <td>0.205817</td>\n",
       "      <td>0.136898</td>\n",
       "      <td>0.102000</td>\n",
       "      <td>0.240987</td>\n",
       "      <td>...</td>\n",
       "      <td>0.139401</td>\n",
       "      <td>0.275825</td>\n",
       "      <td>0.136986</td>\n",
       "      <td>0.157074</td>\n",
       "      <td>0.223055</td>\n",
       "      <td>0.192202</td>\n",
       "      <td>0.190231</td>\n",
       "      <td>0.166329</td>\n",
       "      <td>0.196615</td>\n",
       "      <td>0.175919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 20889 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "inchikey14      LFTLOKWAGJYHHR  BQDXDGDOYPUUOD  VEPUCZUJLKAVNM  \\\n",
       "inchikey14                                                       \n",
       "LFTLOKWAGJYHHR        1.000000        0.057353        0.042969   \n",
       "BQDXDGDOYPUUOD        0.057353        1.000000        0.162866   \n",
       "VEPUCZUJLKAVNM        0.042969        0.162866        1.000000   \n",
       "PXPSEALQIQRPQY        0.053269        0.215026        0.286316   \n",
       "HDZVRBPBPCZCJG        0.069264        0.242169        0.113158   \n",
       "\n",
       "inchikey14      PXPSEALQIQRPQY  HDZVRBPBPCZCJG  SXJIZQPZESTWLD  \\\n",
       "inchikey14                                                       \n",
       "LFTLOKWAGJYHHR        0.053269        0.069264        0.055453   \n",
       "BQDXDGDOYPUUOD        0.215026        0.242169        0.176221   \n",
       "VEPUCZUJLKAVNM        0.286316        0.113158        0.152310   \n",
       "PXPSEALQIQRPQY        1.000000        0.134357        0.185499   \n",
       "HDZVRBPBPCZCJG        0.134357        1.000000        0.145266   \n",
       "\n",
       "inchikey14      JDOFZOKGCYYUER  WGTCMJBJRPKENJ  FCCDDURTIIUXBY  \\\n",
       "inchikey14                                                       \n",
       "LFTLOKWAGJYHHR        0.048193        0.053296        0.052863   \n",
       "BQDXDGDOYPUUOD        0.296270        0.195915        0.089888   \n",
       "VEPUCZUJLKAVNM        0.208607        0.264908        0.082418   \n",
       "PXPSEALQIQRPQY        0.342992        0.344860        0.081680   \n",
       "HDZVRBPBPCZCJG        0.205817        0.136898        0.102000   \n",
       "\n",
       "inchikey14      FDLLEBFMOIHMNM  ...  RJAHLSXSRQXGGI  VKJTXCWIQDBMLE  \\\n",
       "inchikey14                      ...                                   \n",
       "LFTLOKWAGJYHHR        0.056204  ...        0.049612        0.054762   \n",
       "BQDXDGDOYPUUOD        0.460000  ...        0.185547        0.430151   \n",
       "VEPUCZUJLKAVNM        0.186620  ...        0.157480        0.167925   \n",
       "PXPSEALQIQRPQY        0.257655  ...        0.243169        0.248216   \n",
       "HDZVRBPBPCZCJG        0.240987  ...        0.139401        0.275825   \n",
       "\n",
       "inchikey14      NFIHKFSODJJLGC  NHLBOKNHQIEJIH  QABASLXUKXNHMC  \\\n",
       "inchikey14                                                       \n",
       "LFTLOKWAGJYHHR        0.053929        0.060065        0.049683   \n",
       "BQDXDGDOYPUUOD        0.180851        0.218014        0.321244   \n",
       "VEPUCZUJLKAVNM        0.233333        0.152523        0.228311   \n",
       "PXPSEALQIQRPQY        0.351724        0.207021        0.299073   \n",
       "HDZVRBPBPCZCJG        0.136986        0.157074        0.223055   \n",
       "\n",
       "inchikey14      XGVJWXAYKUHDOO  MNKNQKOOKLVXDB  CQKNELOTFUSOTP  \\\n",
       "inchikey14                                                       \n",
       "LFTLOKWAGJYHHR        0.052980        0.049046        0.095833   \n",
       "BQDXDGDOYPUUOD        0.297297        0.272672        0.147776   \n",
       "VEPUCZUJLKAVNM        0.251228        0.226978        0.080844   \n",
       "PXPSEALQIQRPQY        0.376868        0.351122        0.073363   \n",
       "HDZVRBPBPCZCJG        0.192202        0.190231        0.166329   \n",
       "\n",
       "inchikey14      MHCYVCDXRQGUFW  NMCMVEXMLSARCJ  \n",
       "inchikey14                                      \n",
       "LFTLOKWAGJYHHR        0.050964        0.050159  \n",
       "BQDXDGDOYPUUOD        0.317369        0.253207  \n",
       "VEPUCZUJLKAVNM        0.289835        0.231393  \n",
       "PXPSEALQIQRPQY        0.362462        0.374658  \n",
       "HDZVRBPBPCZCJG        0.196615        0.175919  \n",
       "\n",
       "[5 rows x 20889 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46d5437d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04296875"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanis.loc['LFTLOKWAGJYHHR', 'VEPUCZUJLKAVNM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ebf2f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>inchikey14</th>\n",
       "      <th>LFTLOKWAGJYHHR</th>\n",
       "      <th>BQDXDGDOYPUUOD</th>\n",
       "      <th>VEPUCZUJLKAVNM</th>\n",
       "      <th>PXPSEALQIQRPQY</th>\n",
       "      <th>HDZVRBPBPCZCJG</th>\n",
       "      <th>SXJIZQPZESTWLD</th>\n",
       "      <th>JDOFZOKGCYYUER</th>\n",
       "      <th>WGTCMJBJRPKENJ</th>\n",
       "      <th>FCCDDURTIIUXBY</th>\n",
       "      <th>FDLLEBFMOIHMNM</th>\n",
       "      <th>...</th>\n",
       "      <th>RJAHLSXSRQXGGI</th>\n",
       "      <th>VKJTXCWIQDBMLE</th>\n",
       "      <th>NFIHKFSODJJLGC</th>\n",
       "      <th>NHLBOKNHQIEJIH</th>\n",
       "      <th>QABASLXUKXNHMC</th>\n",
       "      <th>XGVJWXAYKUHDOO</th>\n",
       "      <th>MNKNQKOOKLVXDB</th>\n",
       "      <th>CQKNELOTFUSOTP</th>\n",
       "      <th>MHCYVCDXRQGUFW</th>\n",
       "      <th>NMCMVEXMLSARCJ</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inchikey14</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LFTLOKWAGJYHHR</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BQDXDGDOYPUUOD</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VEPUCZUJLKAVNM</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PXPSEALQIQRPQY</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HDZVRBPBPCZCJG</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 20889 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "inchikey14      LFTLOKWAGJYHHR  BQDXDGDOYPUUOD  VEPUCZUJLKAVNM  \\\n",
       "inchikey14                                                       \n",
       "LFTLOKWAGJYHHR             1.0             NaN             NaN   \n",
       "BQDXDGDOYPUUOD             NaN             1.0             NaN   \n",
       "VEPUCZUJLKAVNM             NaN             NaN             1.0   \n",
       "PXPSEALQIQRPQY             NaN             NaN             NaN   \n",
       "HDZVRBPBPCZCJG             NaN             NaN             NaN   \n",
       "\n",
       "inchikey14      PXPSEALQIQRPQY  HDZVRBPBPCZCJG  SXJIZQPZESTWLD  \\\n",
       "inchikey14                                                       \n",
       "LFTLOKWAGJYHHR             NaN             NaN             NaN   \n",
       "BQDXDGDOYPUUOD             NaN             NaN             NaN   \n",
       "VEPUCZUJLKAVNM             NaN             NaN             NaN   \n",
       "PXPSEALQIQRPQY             1.0             NaN             NaN   \n",
       "HDZVRBPBPCZCJG             NaN             1.0             NaN   \n",
       "\n",
       "inchikey14      JDOFZOKGCYYUER  WGTCMJBJRPKENJ  FCCDDURTIIUXBY  \\\n",
       "inchikey14                                                       \n",
       "LFTLOKWAGJYHHR             NaN             NaN             NaN   \n",
       "BQDXDGDOYPUUOD             NaN             NaN             NaN   \n",
       "VEPUCZUJLKAVNM             NaN             NaN             NaN   \n",
       "PXPSEALQIQRPQY             NaN             NaN             NaN   \n",
       "HDZVRBPBPCZCJG             NaN             NaN             NaN   \n",
       "\n",
       "inchikey14      FDLLEBFMOIHMNM  ...  RJAHLSXSRQXGGI  VKJTXCWIQDBMLE  \\\n",
       "inchikey14                      ...                                   \n",
       "LFTLOKWAGJYHHR             NaN  ...             NaN             NaN   \n",
       "BQDXDGDOYPUUOD             NaN  ...             NaN             NaN   \n",
       "VEPUCZUJLKAVNM             NaN  ...             NaN             NaN   \n",
       "PXPSEALQIQRPQY             NaN  ...             NaN             NaN   \n",
       "HDZVRBPBPCZCJG             NaN  ...             NaN             NaN   \n",
       "\n",
       "inchikey14      NFIHKFSODJJLGC  NHLBOKNHQIEJIH  QABASLXUKXNHMC  \\\n",
       "inchikey14                                                       \n",
       "LFTLOKWAGJYHHR             NaN             NaN             NaN   \n",
       "BQDXDGDOYPUUOD             NaN             NaN             NaN   \n",
       "VEPUCZUJLKAVNM             NaN             NaN             NaN   \n",
       "PXPSEALQIQRPQY             NaN             NaN             NaN   \n",
       "HDZVRBPBPCZCJG             NaN             NaN             NaN   \n",
       "\n",
       "inchikey14      XGVJWXAYKUHDOO  MNKNQKOOKLVXDB  CQKNELOTFUSOTP  \\\n",
       "inchikey14                                                       \n",
       "LFTLOKWAGJYHHR             NaN             NaN             NaN   \n",
       "BQDXDGDOYPUUOD             NaN             NaN             NaN   \n",
       "VEPUCZUJLKAVNM             NaN             NaN             NaN   \n",
       "PXPSEALQIQRPQY             NaN             NaN             NaN   \n",
       "HDZVRBPBPCZCJG             NaN             NaN             NaN   \n",
       "\n",
       "inchikey14      MHCYVCDXRQGUFW  NMCMVEXMLSARCJ  \n",
       "inchikey14                                      \n",
       "LFTLOKWAGJYHHR             NaN             NaN  \n",
       "BQDXDGDOYPUUOD             NaN             NaN  \n",
       "VEPUCZUJLKAVNM             NaN             NaN  \n",
       "PXPSEALQIQRPQY             NaN             NaN  \n",
       "HDZVRBPBPCZCJG             NaN             NaN  \n",
       "\n",
       "[5 rows x 20889 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7de2e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
